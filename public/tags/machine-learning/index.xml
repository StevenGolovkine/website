<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning | TwentyFourSecs</title>
    <link>/tags/machine-learning/</link>
      <atom:link href="/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Machine Learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2020 Steven Golovkine. All Rights Reserved</copyright><lastBuildDate>Sat, 05 Jan 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/pic.jpg</url>
      <title>Machine Learning</title>
      <link>/tags/machine-learning/</link>
    </image>
    
    <item>
      <title>Lower Back Pain</title>
      <link>/project/lower-back-pain/</link>
      <pubDate>Sat, 05 Jan 2019 00:00:00 +0000</pubDate>
      <guid>/project/lower-back-pain/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#context&#34;&gt;Context&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#question&#34;&gt;Question&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#exploration-of-the-data&#34;&gt;Exploration of the data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#subset-features-selection&#34;&gt;Subset features selection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-construction&#34;&gt;Model construction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;This dataset is provided by sammy123 on &lt;a href=&#34;https://www.kaggle.com/sammy123/lower-back-pain-symptoms-dataset&#34;&gt;Kaggle&lt;/a&gt;. My study and the complete code are on a Kaggle &lt;a href=&#34;https://www.kaggle.com/stevengolo/lower-back-pain-syndrom&#34;&gt;kernel&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;context&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Context&lt;/h1&gt;
&lt;p&gt;Lower back pain can be caused by a variety of problems with any parts of the complex, interconnected network of spinal muscles, nerves, bones, discs or tendons in the lumbar spines. Typical sources of low back pain include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The large nerve roots in the low back that go to the legs may be irritated.&lt;/li&gt;
&lt;li&gt;The smaller nerves that supply the low back may be irritated.&lt;/li&gt;
&lt;li&gt;The large paired lower back muscles (erector spinae) may be strained.&lt;/li&gt;
&lt;li&gt;The bones, ligaments or joints may be damaged.&lt;/li&gt;
&lt;li&gt;An intervertebral disc may be degenerating.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;An irritation or problem with any of these structures can cause lower back pain and/or pain that radiates or is referred to other parts of the body. Many lower back problems can also cause back muscle spasms, which do not sound like much but can cause severe pain and disability.&lt;/p&gt;
&lt;p&gt;While lower back pain is extremely common, the symptoms and severity of lower back pain vary greatly. A simple lower back muscle strain might be excruciating enough to necessitate an emergency room visit, while a degenerating disc might cause only mild, intermittent discomfort.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;question&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Question&lt;/h1&gt;
&lt;p&gt;How identify an abnormal or normal person using collected physical spine details and data?&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Import the data
data = pd.read_csv(&amp;quot;Dataset_spine.csv&amp;quot;, decimal=&amp;#39;.&amp;#39;, sep=&amp;#39;,&amp;#39;, header=0)
data = data.drop(&amp;#39;Unnamed: 13&amp;#39;, 1)
data.columns = [&amp;#39;pelvic_incidence&amp;#39;, &amp;#39;pelvic_tilt&amp;#39;,
                &amp;#39;lumbar_lordosis_angle&amp;#39;, &amp;#39;sacral_slope&amp;#39;,
                &amp;#39;pelvic_radius&amp;#39;, &amp;#39;degree_spondylolisthesis&amp;#39;,
                &amp;#39;pelvic_slope&amp;#39;, &amp;#39;direct_tilt&amp;#39;,
                &amp;#39;thoracic_slope&amp;#39;, &amp;#39;cervical_tilt&amp;#39;,
                &amp;#39;sacrum_angle&amp;#39;, &amp;#39;scoliosis_slope&amp;#39;,
                &amp;#39;class&amp;#39;]

data.head()&lt;/code&gt;&lt;/pre&gt;
&lt;div style=&#34;overflow-x:auto;&#34;&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table class=&#34;dataframe&#34;&gt;
&lt;thead&gt;
&lt;tr style=&#34;text-align: right;&#34;&gt;
&lt;th&gt;
Pelvic Incidence
&lt;/th&gt;
&lt;th&gt;
Pelvic Tilt
&lt;/th&gt;
&lt;th&gt;
Lumbar Lordosis Angle
&lt;/th&gt;
&lt;th&gt;
Sacral Slope
&lt;/th&gt;
&lt;th&gt;
Pelvic Radius
&lt;/th&gt;
&lt;th&gt;
Degree Spondylolisthesis
&lt;/th&gt;
&lt;th&gt;
Pelvic Slope
&lt;/th&gt;
&lt;th&gt;
Direct Tilt
&lt;/th&gt;
&lt;th&gt;
Thoracic Slope
&lt;/th&gt;
&lt;th&gt;
Cervical Tilt
&lt;/th&gt;
&lt;th&gt;
Sacrum Angle
&lt;/th&gt;
&lt;th&gt;
Scoliosis Slope
&lt;/th&gt;
&lt;th&gt;
Class
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;
63.027818
&lt;/td&gt;
&lt;td&gt;
22.552586
&lt;/td&gt;
&lt;td&gt;
39.609117
&lt;/td&gt;
&lt;td&gt;
40.475232
&lt;/td&gt;
&lt;td&gt;
98.672917
&lt;/td&gt;
&lt;td&gt;
-0.254400
&lt;/td&gt;
&lt;td&gt;
0.744503
&lt;/td&gt;
&lt;td&gt;
12.5661
&lt;/td&gt;
&lt;td&gt;
14.5386
&lt;/td&gt;
&lt;td&gt;
15.30468
&lt;/td&gt;
&lt;td&gt;
-28.658501
&lt;/td&gt;
&lt;td&gt;
43.5123
&lt;/td&gt;
&lt;td&gt;
Abnormal
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
39.056951
&lt;/td&gt;
&lt;td&gt;
10.060991
&lt;/td&gt;
&lt;td&gt;
25.015378
&lt;/td&gt;
&lt;td&gt;
28.995960
&lt;/td&gt;
&lt;td&gt;
114.405425
&lt;/td&gt;
&lt;td&gt;
4.564259
&lt;/td&gt;
&lt;td&gt;
0.415186
&lt;/td&gt;
&lt;td&gt;
12.8874
&lt;/td&gt;
&lt;td&gt;
17.5323
&lt;/td&gt;
&lt;td&gt;
16.78486
&lt;/td&gt;
&lt;td&gt;
-25.530607
&lt;/td&gt;
&lt;td&gt;
16.1102
&lt;/td&gt;
&lt;td&gt;
Abnormal
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
68.832021
&lt;/td&gt;
&lt;td&gt;
22.218482
&lt;/td&gt;
&lt;td&gt;
50.092194
&lt;/td&gt;
&lt;td&gt;
46.613539
&lt;/td&gt;
&lt;td&gt;
105.985135
&lt;/td&gt;
&lt;td&gt;
-3.530317
&lt;/td&gt;
&lt;td&gt;
0.474889
&lt;/td&gt;
&lt;td&gt;
26.8343
&lt;/td&gt;
&lt;td&gt;
17.4861
&lt;/td&gt;
&lt;td&gt;
16.65897
&lt;/td&gt;
&lt;td&gt;
-29.031888
&lt;/td&gt;
&lt;td&gt;
19.2221
&lt;/td&gt;
&lt;td&gt;
Abnormal
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
69.297008
&lt;/td&gt;
&lt;td&gt;
24.652878
&lt;/td&gt;
&lt;td&gt;
44.311238
&lt;/td&gt;
&lt;td&gt;
44.644130
&lt;/td&gt;
&lt;td&gt;
101.868495
&lt;/td&gt;
&lt;td&gt;
11.211523
&lt;/td&gt;
&lt;td&gt;
0.369345
&lt;/td&gt;
&lt;td&gt;
23.5603
&lt;/td&gt;
&lt;td&gt;
12.7074
&lt;/td&gt;
&lt;td&gt;
11.42447
&lt;/td&gt;
&lt;td&gt;
-30.470246
&lt;/td&gt;
&lt;td&gt;
18.8329
&lt;/td&gt;
&lt;td&gt;
Abnormal
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
49.712859
&lt;/td&gt;
&lt;td&gt;
9.652075
&lt;/td&gt;
&lt;td&gt;
28.317406
&lt;/td&gt;
&lt;td&gt;
40.060784
&lt;/td&gt;
&lt;td&gt;
108.168725
&lt;/td&gt;
&lt;td&gt;
7.918501
&lt;/td&gt;
&lt;td&gt;
0.543360
&lt;/td&gt;
&lt;td&gt;
35.4940
&lt;/td&gt;
&lt;td&gt;
15.9546
&lt;/td&gt;
&lt;td&gt;
8.87237
&lt;/td&gt;
&lt;td&gt;
-16.378376
&lt;/td&gt;
&lt;td&gt;
24.9171
&lt;/td&gt;
&lt;td&gt;
Abnormal
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exploration-of-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exploration of the data&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Let’s check if there are some missing values in this dataset.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;data.info()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    &amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;
    RangeIndex: 310 entries, 0 to 309
    Data columns (total 13 columns):
    pelvic_incidence            310 non-null float64
    pelvic_tilt                 310 non-null float64
    lumbar_lordosis_angle       310 non-null float64
    sacral_slope                310 non-null float64
    pelvic_radius               310 non-null float64
    degree_spondylolisthesis    310 non-null float64
    pelvic_slope                310 non-null float64
    direct_tilt                 310 non-null float64
    thoracic_slope              310 non-null float64
    cervical_tilt               310 non-null float64
    sacrum_angle                310 non-null float64
    scoliosis_slope             310 non-null float64
    class                       310 non-null object
    dtypes: float64(12), object(1)
    memory usage: 31.6+ KB&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Compute some basic statistics about the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;data.describe()&lt;/code&gt;&lt;/pre&gt;
&lt;div style=&#34;overflow-x:auto;&#34;&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table class=&#34;dataframe&#34;&gt;
&lt;thead&gt;
&lt;tr style=&#34;text-align: right;&#34;&gt;
&lt;th&gt;
&lt;/th&gt;
&lt;th&gt;
Pelvic Incidence
&lt;/th&gt;
&lt;th&gt;
Pelvic Tilt
&lt;/th&gt;
&lt;th&gt;
Lumbar Lordosis Angle
&lt;/th&gt;
&lt;th&gt;
Sacral Slope
&lt;/th&gt;
&lt;th&gt;
Pelvic Radius
&lt;/th&gt;
&lt;th&gt;
Degree Spondylolisthesis
&lt;/th&gt;
&lt;th&gt;
Pelvic Slope
&lt;/th&gt;
&lt;th&gt;
Direct Tilt
&lt;/th&gt;
&lt;th&gt;
Thoracic Slope
&lt;/th&gt;
&lt;th&gt;
Cervical Tilt
&lt;/th&gt;
&lt;th&gt;
Sacrum Angle
&lt;/th&gt;
&lt;th&gt;
Scoliosis Slope
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;
Count
&lt;/th&gt;
&lt;td&gt;
310.000000
&lt;/td&gt;
&lt;td&gt;
310.000000
&lt;/td&gt;
&lt;td&gt;
310.000000
&lt;/td&gt;
&lt;td&gt;
310.000000
&lt;/td&gt;
&lt;td&gt;
310.000000
&lt;/td&gt;
&lt;td&gt;
310.000000
&lt;/td&gt;
&lt;td&gt;
310.000000
&lt;/td&gt;
&lt;td&gt;
310.000000
&lt;/td&gt;
&lt;td&gt;
310.000000
&lt;/td&gt;
&lt;td&gt;
310.000000
&lt;/td&gt;
&lt;td&gt;
310.000000
&lt;/td&gt;
&lt;td&gt;
310.000000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
Mean
&lt;/th&gt;
&lt;td&gt;
60.496653
&lt;/td&gt;
&lt;td&gt;
17.542822
&lt;/td&gt;
&lt;td&gt;
51.930930
&lt;/td&gt;
&lt;td&gt;
42.953831
&lt;/td&gt;
&lt;td&gt;
117.920655
&lt;/td&gt;
&lt;td&gt;
26.296694
&lt;/td&gt;
&lt;td&gt;
0.472979
&lt;/td&gt;
&lt;td&gt;
21.321526
&lt;/td&gt;
&lt;td&gt;
13.064511
&lt;/td&gt;
&lt;td&gt;
11.933317
&lt;/td&gt;
&lt;td&gt;
-14.053139
&lt;/td&gt;
&lt;td&gt;
25.645981
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
Std
&lt;/th&gt;
&lt;td&gt;
17.236520
&lt;/td&gt;
&lt;td&gt;
10.008330
&lt;/td&gt;
&lt;td&gt;
18.554064
&lt;/td&gt;
&lt;td&gt;
13.423102
&lt;/td&gt;
&lt;td&gt;
13.317377
&lt;/td&gt;
&lt;td&gt;
37.559027
&lt;/td&gt;
&lt;td&gt;
0.285787
&lt;/td&gt;
&lt;td&gt;
8.639423
&lt;/td&gt;
&lt;td&gt;
3.399713
&lt;/td&gt;
&lt;td&gt;
2.893265
&lt;/td&gt;
&lt;td&gt;
12.225582
&lt;/td&gt;
&lt;td&gt;
10.450558
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
Min
&lt;/th&gt;
&lt;td&gt;
26.147921
&lt;/td&gt;
&lt;td&gt;
-6.554948
&lt;/td&gt;
&lt;td&gt;
14.000000
&lt;/td&gt;
&lt;td&gt;
13.366931
&lt;/td&gt;
&lt;td&gt;
70.082575
&lt;/td&gt;
&lt;td&gt;
-11.058179
&lt;/td&gt;
&lt;td&gt;
0.003220
&lt;/td&gt;
&lt;td&gt;
7.027000
&lt;/td&gt;
&lt;td&gt;
7.037800
&lt;/td&gt;
&lt;td&gt;
7.030600
&lt;/td&gt;
&lt;td&gt;
-35.287375
&lt;/td&gt;
&lt;td&gt;
7.007900
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
25%
&lt;/th&gt;
&lt;td&gt;
46.430294
&lt;/td&gt;
&lt;td&gt;
10.667069
&lt;/td&gt;
&lt;td&gt;
37.000000
&lt;/td&gt;
&lt;td&gt;
33.347122
&lt;/td&gt;
&lt;td&gt;
110.709196
&lt;/td&gt;
&lt;td&gt;
1.603727
&lt;/td&gt;
&lt;td&gt;
0.224367
&lt;/td&gt;
&lt;td&gt;
13.054400
&lt;/td&gt;
&lt;td&gt;
10.417800
&lt;/td&gt;
&lt;td&gt;
9.541140
&lt;/td&gt;
&lt;td&gt;
-24.289522
&lt;/td&gt;
&lt;td&gt;
17.189075
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
50%
&lt;/th&gt;
&lt;td&gt;
58.691038
&lt;/td&gt;
&lt;td&gt;
16.357689
&lt;/td&gt;
&lt;td&gt;
49.562398
&lt;/td&gt;
&lt;td&gt;
42.404912
&lt;/td&gt;
&lt;td&gt;
118.268178
&lt;/td&gt;
&lt;td&gt;
11.767934
&lt;/td&gt;
&lt;td&gt;
0.475989
&lt;/td&gt;
&lt;td&gt;
21.907150
&lt;/td&gt;
&lt;td&gt;
12.938450
&lt;/td&gt;
&lt;td&gt;
11.953835
&lt;/td&gt;
&lt;td&gt;
-14.622856
&lt;/td&gt;
&lt;td&gt;
24.931950
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
75%
&lt;/th&gt;
&lt;td&gt;
72.877696
&lt;/td&gt;
&lt;td&gt;
22.120395
&lt;/td&gt;
&lt;td&gt;
63.000000
&lt;/td&gt;
&lt;td&gt;
52.695888
&lt;/td&gt;
&lt;td&gt;
125.467674
&lt;/td&gt;
&lt;td&gt;
41.287352
&lt;/td&gt;
&lt;td&gt;
0.704846
&lt;/td&gt;
&lt;td&gt;
28.954075
&lt;/td&gt;
&lt;td&gt;
15.889525
&lt;/td&gt;
&lt;td&gt;
14.371810
&lt;/td&gt;
&lt;td&gt;
-3.497094
&lt;/td&gt;
&lt;td&gt;
33.979600
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th&gt;
Max
&lt;/th&gt;
&lt;td&gt;
129.834041
&lt;/td&gt;
&lt;td&gt;
49.431864
&lt;/td&gt;
&lt;td&gt;
125.742385
&lt;/td&gt;
&lt;td&gt;
121.429566
&lt;/td&gt;
&lt;td&gt;
163.071041
&lt;/td&gt;
&lt;td&gt;
418.543082
&lt;/td&gt;
&lt;td&gt;
0.998827
&lt;/td&gt;
&lt;td&gt;
36.743900
&lt;/td&gt;
&lt;td&gt;
19.324000
&lt;/td&gt;
&lt;td&gt;
16.821080
&lt;/td&gt;
&lt;td&gt;
6.972071
&lt;/td&gt;
&lt;td&gt;
44.341200
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;No results seem to be unusual, except for the maximum of the &lt;em&gt;Degree Spondylolisthesis&lt;/em&gt;. Usually, a degree is between -180° and 180° (or 0° and 360°). If we look at the other data, it seems that the coding of the angle is between -180° and 180° (with a very few negative angle). Let’s look at all the values out of the usual range of the degrees (it concerns only the variable &lt;em&gt;Degree Spondylolisthesis&lt;/em&gt;).&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;data[data.degree_spondylolisthesis &amp;gt; 180]&lt;/code&gt;&lt;/pre&gt;
&lt;div style=&#34;overflow-x:auto;&#34;&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table class=&#34;dataframe&#34;&gt;
&lt;thead&gt;
&lt;tr style=&#34;text-align: right;&#34;&gt;
&lt;th&gt;
&lt;/th&gt;
&lt;th&gt;
Pelvic Incidence
&lt;/th&gt;
&lt;th&gt;
Pelvic Tilt
&lt;/th&gt;
&lt;th&gt;
Lumbar Lordosis Angle
&lt;/th&gt;
&lt;th&gt;
Sacral Slope
&lt;/th&gt;
&lt;th&gt;
Pelvic Radius
&lt;/th&gt;
&lt;th&gt;
Degree Spondylolisthesis
&lt;/th&gt;
&lt;th&gt;
Pelvic Slope
&lt;/th&gt;
&lt;th&gt;
Direct Tilt
&lt;/th&gt;
&lt;th&gt;
Thoracic Slope
&lt;/th&gt;
&lt;th&gt;
Cervical Tilt
&lt;/th&gt;
&lt;th&gt;
Sacrum Angle
&lt;/th&gt;
&lt;th&gt;
Scoliosis Slope
&lt;/th&gt;
&lt;th&gt;
Class
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;th&gt;
115
&lt;/th&gt;
&lt;td&gt;
129.834041
&lt;/td&gt;
&lt;td&gt;
8.404475
&lt;/td&gt;
&lt;td&gt;
48.384057
&lt;/td&gt;
&lt;td&gt;
121.429566
&lt;/td&gt;
&lt;td&gt;
107.690466
&lt;/td&gt;
&lt;td&gt;
418.543082
&lt;/td&gt;
&lt;td&gt;
0.860223
&lt;/td&gt;
&lt;td&gt;
18.5943
&lt;/td&gt;
&lt;td&gt;
11.1514
&lt;/td&gt;
&lt;td&gt;
11.36543
&lt;/td&gt;
&lt;td&gt;
-34.202073
&lt;/td&gt;
&lt;td&gt;
27.5144
&lt;/td&gt;
&lt;td&gt;
Abnormal
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Only one observation has a &lt;em&gt;Degree Spondylolisthesis&lt;/em&gt; larger than 180. We can consider a typo in the decimal of this value. So, we replace the value 418.543082 by 41.8543082.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;data.loc[115, &amp;#39;degree_spondylolisthesis&amp;#39;] = 41.8543082&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Recode the variable &lt;em&gt;class&lt;/em&gt; into a dummy variable (0: Abnormal, 1: Normal).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;data[&amp;#39;class&amp;#39;] = pd.get_dummies(data[&amp;#39;class&amp;#39;], prefix=&amp;#39;class&amp;#39;, drop_first=True)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Then, we look at the correlation between the different variables.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Compute the correlation matrix.
corr_data = round(data.corr(),2)&lt;/code&gt;&lt;/pre&gt;
&lt;center&gt;
&lt;img src=&#34;output_15_0.png&#34; title=&#34;fig:&#34; alt=&#34;png&#34; /&gt;
&lt;/center&gt;
&lt;p&gt;So, it appears that the class {Abnormal, Normal} is negatively correlated with the &lt;em&gt;Pelvic Incidence&lt;/em&gt;, the &lt;em&gt;Pelvic Tilt&lt;/em&gt;, the &lt;em&gt;Lumbar Lordosis Angle&lt;/em&gt;, the &lt;em&gt;Sacral Slope&lt;/em&gt; and the &lt;em&gt;Degree Spondylolisthesis&lt;/em&gt; and positively correlated with the &lt;em&gt;Pelvic Radius&lt;/em&gt;. The class has a very small correlation with the other variables.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Let’s look at some boxplot for these variables.&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;
&lt;img src=&#34;output_18_0.png&#34; title=&#34;fig:&#34; alt=&#34;png&#34; /&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;div id=&#34;subset-features-selection&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Subset features selection&lt;/h1&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model = ExtraTreesClassifier(n_estimators=200, random_state=0)
model.fit(data.drop(&amp;#39;class&amp;#39;, axis=1, inplace=False), data[&amp;#39;class&amp;#39;])

importances = model.feature_importances_
importances_std = np.std([model_tree.feature_importances_ for model_tree in model.estimators_], axis=0)&lt;/code&gt;&lt;/pre&gt;
&lt;center&gt;
&lt;img src=&#34;output_21_0.png&#34; title=&#34;fig:&#34; alt=&#34;png&#34; /&gt;
&lt;/center&gt;
&lt;p&gt;So, we have an importance score for each attribute where the larger score the more important the attribute. As we see on the correlation plot, the variable &lt;em&gt;degree spondylolisthesis&lt;/em&gt; and &lt;em&gt;pelvic radius&lt;/em&gt;/&lt;em&gt;pelvic tilt&lt;/em&gt;/&lt;em&gt;pelvic incidence&lt;/em&gt;/&lt;em&gt;lumbar lordosis angle&lt;/em&gt; are strongly correlated. We will consider only the variables &lt;em&gt;Degree Spondylolisthesis&lt;/em&gt;, &lt;em&gt;Pelvic Radius&lt;/em&gt;, &lt;em&gt;Pelvic Tilt&lt;/em&gt; and &lt;em&gt;Pelvic Incidence&lt;/em&gt; for building the model (the four with the strongest importance).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Let’s plot these variables with the class.&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;
&lt;img src=&#34;output_24_0.png&#34; title=&#34;fig:&#34; alt=&#34;png&#34; /&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;div id=&#34;model-construction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Model construction&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Split the dataset into train and test set.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;X_train, X_test, Y_train, Y_test = train_test_split(data[[&amp;#39;degree_spondylolisthesis&amp;#39;, &amp;#39;pelvic_radius&amp;#39;, &amp;#39;pelvic_tilt&amp;#39;, &amp;#39;pelvic_incidence&amp;#39;]], data[&amp;#39;class&amp;#39;], test_size=1/3, random_state=42)

scaler = StandardScaler().fit(X_train)
X_train_transformed = scaler.transform(X_train)
X_test_transformed = scaler.transform(X_test)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Let’s construct the baseline by setting the most frequent response in the training set to compare our model.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dummy = DummyClassifier(strategy=&amp;#39;most_frequent&amp;#39;, random_state=42)
dummy.fit(X_train_transformed, Y_train)
Y_pred_dummy = dummy.predict(X_test_transformed)

Y_pred_proba_dummy = dummy.predict_proba(X_test_transformed)[:, 1]
[fpr_dummy, tpr_dummy, thr_dummy] = metrics.roc_curve(Y_test, Y_pred_proba_dummy)&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;The accuracy for the dummy classifier is 72%.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Use the Logistic Regression method to predict the class (by Cross-Validation and GridSearch).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;param_log_reg = {&amp;#39;tol&amp;#39;: np.logspace(-5, 1, 7),
                 &amp;#39;C&amp;#39;: np.logspace(-3, 3, 7),
                 &amp;#39;penalty&amp;#39;: [&amp;#39;l2&amp;#39;]}

log_reg = GridSearchCV(LogisticRegression(solver=&amp;#39;lbfgs&amp;#39;), param_log_reg, cv=10, iid=False)
log_reg.fit(X_train_transformed, Y_train)&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;Best parameters set found on development set: {‘C’: 1.0, ‘penalty’: ‘l2’, ‘tol’: 1.0}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;Y_pred_log_reg = log_reg.predict(X_test_transformed)

Y_pred_proba_log_reg = log_reg.predict_proba(X_test_transformed)[:, 1]
[fpr_log_reg, tpr_log_reg, thr_log_reg] = metrics.roc_curve(Y_test, Y_pred_proba_log_reg)&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;The accuracy for the Logistic Regression classifier is 85%.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Plot the ROC curves for each models&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;
&lt;img src=&#34;output_34_0.png&#34; title=&#34;fig:&#34; alt=&#34;png&#34; /&gt;
&lt;/center&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Titanic: Machine Learning from Disaster</title>
      <link>/project/titanic/</link>
      <pubDate>Thu, 22 Nov 2018 00:00:00 +0000</pubDate>
      <guid>/project/titanic/</guid>
      <description>&lt;h2 id=&#34;context&#34;&gt;Context&lt;/h2&gt;
&lt;p&gt;The sinking of the RMS Titanic is one of the most infamous shipwrecks in
history. On April 15, 1912, during her maiden voyage, the Titanic sank
after colliding with an iceberg, killing 1502 out of 2224 passengers and
crew. This sensational tragedy shocked the international community and
led to better safety regulations for ships.&lt;/p&gt;
&lt;p&gt;One of the reasons that the shipwreck led to such loss of life was that
there were not enough lifeboats for the passengers and crew. Although
there was some element of luck involved in surviving the sinking, some
groups of people were more likely to survive than others, such as women,
children, and the upper-class.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;variables-description&#34;&gt;Variables description&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Variable Name&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;PassengerId&lt;/td&gt;
&lt;td&gt;Passenger’s Id&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Survived&lt;/td&gt;
&lt;td&gt;Survived (1) or died (0)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pclass&lt;/td&gt;
&lt;td&gt;Passenger’s class&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Name&lt;/td&gt;
&lt;td&gt;Passenger’s name&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sex&lt;/td&gt;
&lt;td&gt;Passenger’s sex&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Age&lt;/td&gt;
&lt;td&gt;Passenger’s age&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SibSp&lt;/td&gt;
&lt;td&gt;Number of siblings/spouses aboard&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Parch&lt;/td&gt;
&lt;td&gt;Number of parents/children aboard&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ticket&lt;/td&gt;
&lt;td&gt;Ticket number&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Fare&lt;/td&gt;
&lt;td&gt;Passenger Fare&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cabin&lt;/td&gt;
&lt;td&gt;Cabin&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Embarked&lt;/td&gt;
&lt;td&gt;Port of embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;SPECIAL NOTES&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Pclass&lt;/em&gt; is a proxy for socio-economic status (SES): 1st ~ Upper; 2nd
~ Middle; 3rd ~ Lower.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Age&lt;/em&gt; is in Years; fractional if &lt;em&gt;Age&lt;/em&gt; is less than One (1). If the
&lt;em&gt;Age&lt;/em&gt; is estimated, it is in the form xx.5.&lt;/p&gt;
&lt;p&gt;With respect to the family relation variables (i.e. &lt;em&gt;sibsp&lt;/em&gt; and &lt;em&gt;parch&lt;/em&gt;)
some relations were ignored. The following are the definitions used for
&lt;em&gt;sibsp&lt;/em&gt; and &lt;em&gt;parch&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sibling: Brother, Sister, Stepbrother, or Stepsister of Passenger
Aboard Titanic;&lt;/li&gt;
&lt;li&gt;Spouse: Husband or Wife of Passenger Aboard Titanic (Mistresses and
Fiances Ignored);&lt;/li&gt;
&lt;li&gt;Parent: Mother or Father of Passenger Aboard Titanic;&lt;/li&gt;
&lt;li&gt;Child: Son, Daughter, Stepson, or Stepdaughter of Passenger Aboard
Titanic.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other family relatives excluded from this study include cousins,
nephews/nieces, aunts/uncles, and in-laws. Some children travelled only
with a nanny, therefore &lt;em&gt;parch&lt;/em&gt; = 0 for them. As well, some travelled
with very close friends or neighbors in a village, however, the
definitions do not support such relations.&lt;/p&gt;
&lt;h2 id=&#34;load-the-data&#34;&gt;Load the data&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;train &amp;lt;- read_csv(&#39;train.csv&#39;)
test &amp;lt;- read_csv(&#39;test.csv&#39;)

titanic &amp;lt;- train %&amp;gt;% 
            bind_rows(test) %&amp;gt;%
            select(-PassengerId) %&amp;gt;%
            mutate_at(vars(Pclass, Sex, Embarked), funs(factor(.)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The train dataset has 891 observations and 12 variables. The test
dataset has 418 observations and 11 variables. We want to use the
&lt;code&gt;train&lt;/code&gt; dataset to learn if a passenger survived given the different
variables, and then predict the fate of the passenger into the &lt;code&gt;test&lt;/code&gt;
dataset.&lt;/p&gt;
&lt;h2 id=&#34;exploratory-data-analysis&#34;&gt;Exploratory Data Analysis&lt;/h2&gt;
&lt;h3 id=&#34;passengers-class&#34;&gt;Passenger’s class&lt;/h3&gt;
&lt;p&gt;There is no missing values into the &lt;em&gt;PClass&lt;/em&gt; variable. Half of the
passenger are in the third class.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;class-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;passengers-sex&#34;&gt;Passenger’s sex&lt;/h3&gt;
&lt;p&gt;There is almost twice men than women.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;sex-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;passengers-name&#34;&gt;Passenger’s name&lt;/h3&gt;
&lt;p&gt;This variable, obviously, confirm the high number of men compare to the
number of women. But it carry another piece of information: more than
the half of the women on the Titanic are not married (the &lt;code&gt;Miss&lt;/code&gt;
factor). It is probably due to the children.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Extract the title from the Passenger&#39;s name.
Title &amp;lt;- &amp;quot;^.*, (.*?)\\..*$&amp;quot; %&amp;gt;% 
          gsub(&amp;quot;\\1&amp;quot;, titanic$Name)
# Create another factors for low represented title.
title_high &amp;lt;- c(&#39;Mr&#39;, &#39;Miss&#39;, &#39;Mrs&#39;, &#39;Master&#39;)
Title &amp;lt;- Title %in% title_high %&amp;gt;%
          if_else(Title, &#39;Other&#39;)
# Add titlecolumn to the dataframe
titanic &amp;lt;- titanic %&amp;gt;% 
            add_column(Title) %&amp;gt;%
            mutate_at(vars(Title), funs(factor(.)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;name-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;passengers-port-of-embarkation&#34;&gt;Passenger’s port of embarkation&lt;/h3&gt;
&lt;p&gt;1 % of the passengers embarked in Southampton. We do not known the port
of embarkation for only 2 persons. So, we will try to infer these
missing values.&lt;/p&gt;
&lt;p&gt;First, let’s take a look at the 2 passengers with missing port of
embarkation.&lt;/p&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Survived
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Pclass
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Name
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Sex
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Age
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
SibSp
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Parch
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Ticket
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Fare
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Cabin
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Embarked
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Title
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Icard, Miss. Amelie
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
38
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
113572
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
80
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
B28
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Miss
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Stone, Mrs. George Nelson (Martha Evelyn)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
62
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
113572
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
80
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
B28
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mrs
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Miss. Icard and Mrs. Stone paid 80$ and was in first class. Let’s plot a
boxplot to determine the median fare depending on the port of
embarkation for the first class.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;plot_missing_embarked-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;There are only 3 passengers that embarked in Queenstown in first class.
There fare was 90$. Moreover, they were part of the same family. So,
considering the boxplot, we might think that the port of embarkation of
Miss. Icard and Mrs. Stone were Cherbourg.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;titanic[62, &amp;quot;Embarked&amp;quot;] &amp;lt;- &amp;quot;C&amp;quot;
titanic[830, &amp;quot;Embarked&amp;quot;] &amp;lt;- &amp;quot;C&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;port-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;passengers-fare&#34;&gt;Passenger’s fare&lt;/h3&gt;
&lt;p&gt;There is only 1 person with a missing in the all dataset. The mean fare
is 33$ and the median fare 14$ for a ticket on the Titanic. Let’s look
at the person with a missing fare.&lt;/p&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Survived
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Pclass
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Name
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Sex
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Age
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
SibSp
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Parch
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Ticket
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Fare
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Cabin
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Embarked
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Title
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Storey, Mr. Thomas
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
male
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3701
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
S
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mr
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Let’s plot a kernel density estimator of the fare for the person with
the same characteristics than Mr. Storey (embarked in Southampton in
third class).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;plot_fare-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The median for the third class and the embarkment in Southampton is 8$.
So, we might think that Mr. Storey has paid the median fare of the
people from the third class who embarked in Southampton.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;titanic[1044, &amp;quot;Fare&amp;quot;] &amp;lt;- titanic %&amp;gt;% filter(Embarked == &#39;S&#39;, Pclass == 3) %&amp;gt;% pull(Fare) %&amp;gt;% median(na.rm = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;passengers-age&#34;&gt;Passenger’s age&lt;/h3&gt;
&lt;p&gt;There are 263 persons without &lt;em&gt;Age&lt;/em&gt; in the dataset. The mean age is 29.9
years old.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;plot_age-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Since there are a lot of missing values, we are going to input these
ones using a ridge regression (
&lt;a href=&#34;https://cran.r-project.org/web/packages/glmnet/glmnet.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;glmnet_
package&lt;/a&gt;
).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Split the dataset into the ones with Age and the ones without Age.
titanic.with.age &amp;lt;- titanic %&amp;gt;% 
  filter(!is.na(Age)) %&amp;gt;%
  select(-c(Survived, Name, Ticket, Cabin))
titanic.without.age &amp;lt;- titanic %&amp;gt;%
  filter(is.na(Age)) %&amp;gt;%
  select(-c(Survived, Name, Ticket, Cabin)) %&amp;gt;%
  mutate(Age = 0)

# Build a model matrix of the data
titanic.lm &amp;lt;- lm(Age ~ ., data = titanic.with.age)
titanic.with.age.model.matrix &amp;lt;- model.matrix(titanic.lm, data = titanic.with.age)[,-1]
# Perform the Ridge Regression (alpha = 0)
titanic.age.model &amp;lt;- glmnet(titanic.with.age.model.matrix, titanic.with.age$Age, alpha = 0)

# Prediction of the Age 
titanic.without.age$Age &amp;lt;- predict(titanic.age.model, 
  newx = model.matrix(titanic.lm, data = titanic.without.age)[, -1],
  s = cv.glmnet(titanic.with.age.model.matrix, titanic.with.age$Age, alpha = 0)$lambda.min,
  type = &#39;link&#39;)

# Replace the missing Age into the all dataset
titanic[is.na(titanic$Age), &amp;quot;Age&amp;quot;] &amp;lt;- titanic.without.age$Age
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s check the new density estimator for the &lt;em&gt;Age&lt;/em&gt; to ensure that
things still look good. (Careful, one person with a predicted negative
age!)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;plot_age2-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;number-of-siblingsspouses-aboard&#34;&gt;Number of siblings/spouses aboard&lt;/h3&gt;
&lt;p&gt;There is no missing value for the variable &lt;em&gt;SipSp&lt;/em&gt; in the dataset. A
majority if the passengers does not have siblings or spouses aboard.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;plot_sipsp-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;number-of-parentschildren-aboard&#34;&gt;Number of parents/children aboard&lt;/h3&gt;
&lt;p&gt;There is no missing value for the variable &lt;em&gt;Parch&lt;/em&gt; in the dataset. A
majority if the passengers does not have parents or children aboard.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;plot_parch-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;passengers-cabin&#34;&gt;Passenger’s cabin&lt;/h3&gt;
&lt;p&gt;There are 1014 missing values for the &lt;em&gt;Cabin&lt;/em&gt; variable. So, 77% of the
observations are missing. We decided to delete this features from the
dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;titanic &amp;lt;- titanic %&amp;gt;% select(-Cabin)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;passengers-ticket&#34;&gt;Passenger’s ticket&lt;/h3&gt;
&lt;p&gt;There are 0 missing values for the &lt;em&gt;Ticket&lt;/em&gt; variable. But, there are 929
different values. Thus, we also delete this feature from the dataset
because almost every passenger has a different Ticket.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;titanic &amp;lt;- titanic %&amp;gt;% select(-Ticket)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;prediction-of-the-survivors&#34;&gt;Prediction of the survivors&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;train &amp;lt;- titanic %&amp;gt;% select(-Name) %&amp;gt;% filter(!is.na(Survived))
test &amp;lt;- titanic %&amp;gt;% select(-Name) %&amp;gt;% filter(is.na(Survived))

# Split the train set into two dataset (for validation)
set.seed(42)
sample &amp;lt;- sample(c(TRUE, FALSE), nrow(train), replace = TRUE, prob = c(2/3, 1/3))
train.val &amp;lt;- train[sample, ]
test.val &amp;lt;- train[!sample, ]

# Perform Ridge regression
train.lm &amp;lt;- lm(Survived ~ ., data = train.val)
X &amp;lt;- model.matrix(train.lm, data = train.val)[ , -1]
Y &amp;lt;- train.val$Survived
train.ridge.model &amp;lt;- glmnet(X, Y, alpha = 0, family = &#39;binomial&#39;)

# Prediction on the test.val set
test.val.predict &amp;lt;- predict(train.ridge.model, 
                            s = cv.glmnet(X, Y, alpha = 0)$lambda.min,
                            newx = model.matrix(train.lm, data = test.val)[ , -1],
                            type = &#39;class&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On the validation set, there are 0.14% of missclassified passengers.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Prediction of the test set
test$Survived &amp;lt;- 0
test.predict &amp;lt;- predict(train.ridge.model, 
                        s = cv.glmnet(X, Y, alpha = 0)$lambda.min,
                        newx = model.matrix(train.lm, data = test)[ , -1],
                        type = &#39;class&#39;) 

# Construt the dataframe
result &amp;lt;- data.frame(PassengerID = row.names(test.predict),
                     Survived = test.predict[ , 1])

# Export as CSV
write.csv(result, &#39;results.csv&#39;, row.names = FALSE)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
