<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hadoop | TwentyFourSecs</title>
    <link>/categories/hadoop/</link>
      <atom:link href="/categories/hadoop/index.xml" rel="self" type="application/rss+xml" />
    <description>Hadoop</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2020 Steven Golovkine. All Rights Reserved</copyright><lastBuildDate>Sat, 08 Dec 2018 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/pic.jpg</url>
      <title>Hadoop</title>
      <link>/categories/hadoop/</link>
    </image>
    
    <item>
      <title>Set up Hadoop</title>
      <link>/post/hadoop/</link>
      <pubDate>Sat, 08 Dec 2018 00:00:00 +0000</pubDate>
      <guid>/post/hadoop/</guid>
      <description>&lt;h2 id=&#34;hadoop-deployment-modes&#34;&gt;Hadoop deployment modes&lt;/h2&gt;
&lt;p&gt;There are three ways to deploy Hadoop:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Local mode&lt;/li&gt;
&lt;li&gt;Pseudo-distributed mode&lt;/li&gt;
&lt;li&gt;Distributed mode&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;requisites-to-the-installation&#34;&gt;Requisites to the installation&lt;/h2&gt;
&lt;h3 id=&#34;java&#34;&gt;Java&lt;/h3&gt;
&lt;p&gt;Check if &lt;strong&gt;Java&lt;/strong&gt; is installed:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;java -version
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It should return something like that:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;java version &amp;quot;1.8.0_***&amp;quot;
Java(TM) SE Runtime Environment (build 1.8.0_***-b11)
Java HotSpot(TM) 64-Bit Server VM (build 25.***-b11, mixed mode)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If not, you could go on 
&lt;a href=&#34;https://www.java.com/fr/download/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;java.com&lt;/a&gt;
 and download it.&lt;/p&gt;
&lt;h3 id=&#34;ssh&#34;&gt;SSH&lt;/h3&gt;
&lt;p&gt;On MacOS, the &lt;strong&gt;Remote Login&lt;/strong&gt; must be enable to authorise SSH. It is located in &lt;strong&gt;Systeme Preference&lt;/strong&gt; and &lt;strong&gt;Sharing&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Try to ssh to &lt;em&gt;localhost&lt;/em&gt; without a passphrase/password. This is important because we do not want to enter a passphrase/password every time Hadoop connect to a node.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ssh localhost
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you can not, run these commands to create a key and put it into the authorised one to connect.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ssh-keygen -t rsa -P &amp;quot;&amp;quot;
cat ~/.ssh/id_rsa.pub &amp;gt;&amp;gt; ~/.ssh/authorized_keys
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;set-up-on-macos&#34;&gt;Set up on MacOs&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s install Hadoop with Homebrew.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;brew install hadoop
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a result, we see where are Hadoop&amp;rsquo;s config files:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/usr/local/opt/hadoop/libexec/etc/hadoop/hadoop-env.sh
/usr/local/opt/hadoop/libexec/etc/hadoop/mapred-env.sh
/usr/local/opt/hadoop/libexec/etc/hadoop/yarn-env.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Moreover, the &lt;code&gt;JAVA_HOME&lt;/code&gt; has been set to the result of the command &lt;code&gt;/usr/libexec/java_home&lt;/code&gt;.
And finally, the Hadoop&amp;rsquo;s files are in the folder &lt;code&gt;/usr/local/Cellar/hadoop/*&lt;/code&gt;.
Now, in order to simplify the commands, it is common to add some environment variables to the &lt;code&gt;.bashrc&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Hadoop environment
export HADOOP_HOME=/usr/local/Cellar/hadoop/*/libexec
PATH=$HADOOP_HOME/bin:$PATH
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;configure-hdfs-for-the-pseudo-distributed-mode&#34;&gt;Configure HDFS for the Pseudo-Distributed mode&lt;/h3&gt;
&lt;h4 id=&#34;use-a-single-datanode-for-each-block&#34;&gt;Use a single DataNode for each block&lt;/h4&gt;
&lt;p&gt;Add the following lines to the file &lt;code&gt;$HADOOP_HOME/etc/hadoop/hdfs-site.xml&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;configuration&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;dfs.replication&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;1&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;configure-the-namenode-port&#34;&gt;Configure the NameNode port&lt;/h4&gt;
&lt;p&gt;Add the following lines to the file &lt;code&gt;$HADOOP_HOME/etc/hadoop/core-site.xml&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;configuration&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;hdfs://localhost:9000&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;set-the-runtime-framework-for-executing-mapreduce-jobs&#34;&gt;Set the runtime framework for executing MapReduce jobs&lt;/h4&gt;
&lt;p&gt;Add the following lines to the file &lt;code&gt;$HADOOP_HOME/etc/hadoop/mapred-site.xml&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;configuration&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;mapreduce.framework.name&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;yarn&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;implement-the-service-_mapreduce_shuffle_&#34;&gt;Implement the service &lt;em&gt;mapreduce_shuffle&lt;/em&gt;.&lt;/h4&gt;
&lt;p&gt;Add the following lines to the file &lt;code&gt;$HADOOP_HOME/etc/hadoop/yarn-site.xml&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;configuration&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;yarn.nodemanager.aux-services&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;mapreduce_shuffle&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;format-the-filesystem&#34;&gt;Format the filesystem&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;hdfs namenode -format
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, you can start the NameNode and DataNode deamons.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$HADOOP_HOME/sbin/start-dfs.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is possible to check if it&amp;rsquo;s working using the UI interface: 
&lt;a href=&#34;http://localhost:50070/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://localhost:50070/&lt;/a&gt;
 or 
&lt;a href=&#34;http://localhost:9870/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://localhost:9870/&lt;/a&gt;
.&lt;/p&gt;
&lt;p&gt;And start the Ressource and Node managers.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$HADOOP_HOME/sbin/start-yarn.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is possible to check if it&amp;rsquo;s working using the UI interface: 
&lt;a href=&#34;http://localhost:8088/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://localhost:8088/&lt;/a&gt;
.&lt;/p&gt;
&lt;h2 id=&#34;common-errors&#34;&gt;Common Errors&lt;/h2&gt;
&lt;h3 id=&#34;incompatible-clusterids&#34;&gt;Incompatible clusterIDs&lt;/h3&gt;
&lt;p&gt;You should reformat the name node with the right clusterId.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;hdfs namenode -format -clusterId CID-...
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
