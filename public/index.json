[{"authors":["admin"],"categories":null,"content":"Steven Golovkine is a PhD student of statistics at Groupe Renault. He depends on the ENSAI lab. His research interests include functional data analysis, non-parametric statistics and machine learning. His thesis supervisors are Mr Valentin Patilea (CREST, link ) and Mr Nicolas Klutchnikoff (IRMAR, link ).\nHe also loves playing basketball.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Steven Golovkine is a PhD student of statistics at Groupe Renault. He depends on the ENSAI lab. His research interests include functional data analysis, non-parametric statistics and machine learning. His thesis supervisors are Mr Valentin Patilea (CREST, link ) and Mr Nicolas Klutchnikoff (IRMAR, link ).\nHe also loves playing basketball.","tags":null,"title":"Steven Golovkine","type":"authors"},{"authors":null,"categories":["Julia","Programming Language","Set up"],"content":"Installation  Julia can be installed on MacOS using Homebrew.\nbrew cask install julia  Jupyter kernel The installation of the Julia kernel for Jupyter is straightforward following this link .\nOn MacOS, from a Terminal, run julia to launch a Julia session. Then, run the following commands:\nusing Pkg Pkg.add(\u0026quot;IJulia\u0026quot;)  ","date":1577664000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577664000,"objectID":"457b2401468a0a853e4e0cc203f5b527","permalink":"/post/set-up-julia/","publishdate":"2019-12-30T00:00:00Z","relpermalink":"/post/set-up-julia/","section":"post","summary":"Julia is a high-level, high-performance, dynamic programming language. It is used for high-performance numerical analysis and computational science.","tags":["Julia","Programming Language","Set up"],"title":"Set up Julia","type":"post"},{"authors":null,"categories":["Deep Learning","Python"],"content":"Introduction to Pretrained Models for Computer Vision This notebook is based on the Deep Learning course from the Master Datascience Paris Saclay. Materials of the course can be found here .\nIt aims to get some hands-on experience with pre-trained Keras models are reasonably close to the state-of-the-art of some computer vision tasks. The models are pre-trained on large publicly available labeled images datasets such as ImageNet and COCO .\nThis notebook will highlights two specific tasks (or at least try):\n Image Classification: Predict only one class label per-image (assuming a single centered object or image class). Object detection and instance segmentation: Detect and localise all occurrences of objects of a predefined list of classes of interest in a given image.  # Display figure in the notebook %matplotlib inline  # Load packages import cv2 import matplotlib.pyplot as plt import numpy as np import os import zipfile from keras import backend from keras.applications import inception_resnet_v2, mobilenet, resnet50 from skimage.io import imread from skimage.transform import resize from time import time from urllib.request import urlretrieve  Working with images data For the beginning, we will see how to work with images data, how they are represented in memory, how to load it, how to modify it, ect.\nLet\u0026rsquo;s use the library scikit-image to load the content of a JPEG file into a numpy array.\npic = imread('laptop.jpeg')  The type of the variable is: \u0026lt;class 'imageio.core.util.Array'\u0026gt; (which is a particular class of np.ndarray).  An image is described by three parameters: its height; its weight; its color channels (RGB). Each of them corresponds to a dimension of the array.\nThe shape of the picture is (450, 800, 3).  For efficiency reasons, the pixel intensities of each channel are stored as 8-bit unsigned integer taking values in the $[0, 255]$ range.\nThe type of the elements in the array is uint8, the minimum value of the pixels is 0 and the maximum value is 255.  # Look at the picture fig = plt.imshow(pic) fig.axes.get_xaxis().set_visible(False) fig.axes.get_yaxis().set_visible(False) plt.show()  The size in bytes of a Numpy array can be computed by multiplying the number of elements by the size in byte of each element in the array. The size of one element depend of the data type.\nUsing the shape, the size of the image is 1.08MB, the computation is 450 (height) * 800 (weight) * 3 (channel) * 8 (# bits to represents one element) / 8 (# bits in one byte). We can check this results using the function `nbytes`: 1.08MB.  Indexing on the last dimension makes it possible to extract the 2D content of a specific color channel. Consider the following example for the red channel.\npic_red = pic[:, :, 0]  fig = plt.imshow(pic_red, cmap=plt.cm.Reds_r) fig.axes.get_xaxis().set_visible(False) fig.axes.get_yaxis().set_visible(False) plt.show()  Now, we consider the grey-level version of the image with shape (height, weight). To compute this version, we compute the mean of each pixel values across the channels.\npic_grey = pic.mean(axis=2)  fig = plt.imshow(pic_grey, cmap=plt.cm.Greys_r) fig.axes.get_xaxis().set_visible(False) fig.axes.get_yaxis().set_visible(False) plt.show()  The uint8 integer data type can not represent the grey pixels because they have floating points. Anyway, Numpy represents it as float64 data type.\nThe size of the grey picture is 2.88MB (which is higher than the color picture, it is due to the `float` data type.  The expected of range values for the new pixels is the same as before, $[0, 255]$ (0 for white pixels and 255 for black ones).\nResizing images, handling data types and dynamic ranges When dealing with an heterogeneous collection of image of various sizes, it is often necessary to resize the image to the same size. More specifically:\n for image classification, most networks expect a specific fixed input size; for object detection and instance segmentation, networks have more flexibility but the images should have approximately the same size as the training set images.  Furthermore, large images can be much slower to process than smaller images. This is due to the fact that the number of pixels varies quadratically with the height and width.\n# Resize the picture to have a 50*50 picture. pic = imread('laptop.jpeg') pic_lowres = resize(pic, output_shape=(50, 50), mode='reflect', anti_aliasing=True)  fig = plt.imshow(pic_lowres, interpolation='nearest') fig.axes.get_xaxis().set_visible(False) fig.axes.get_yaxis().set_visible(False) plt.show()  The values of the pixels of the low resolution image are computed by combining the values of the pixels in the high resolution image. The result is therefore represented as floating points.\nThe type of the elements of the array is float64, and the size of the image is 0.06MB.  Careful, by conventions, both skimage.transform.imresize and plt.imshow assume that floating point values range from $0.0$ to $1.0$ when using floating points as opposed to $0$ to $255$ when using 8-bit integers.\nSo, the range of pixel values of the low resolution image is [0.0, 0.996].  Note that Keras on the other hand might expect images encoded with values in the $[0.0, 255.0]$ range irrespectively of the data type of the array. To avoid the implicit conversion to the $[0.0, 1.0]$ range, we can use the preserve_range=True option in the resize function. But the dtype will change to float64.\npic_lowres_larran = resize(pic, output_shape=(50, 50), mode='reflect', anti_aliasing=True, preserve_range=True)  And, the range of pixel values of the low resolution image with `preserve_range=True` is [0.0, 254.0].  Warning: The behavior of plt.imshow depends on both the dtype and the dynamic range when displaying RGB images. In particular, it does not work on RGB images with float64 values in the $[0.0, 255.0]$ range.\nfig = plt.imshow(pic_lowres_larran, interpolation='nearest') fig.axes.get_xaxis().set_visible(False) fig.axes.get_yaxis().set_visible(False) plt.show()  For correctly displaying an RGB array with floating point values in the $[0.0, 255.0]$ range, we can divide all the values by $255$ or change the dtype to integers.\nfig = plt.imshow(pic_lowres_larran / 255.0, interpolation='nearest') fig.axes.get_xaxis().set_visible(False) fig.axes.get_yaxis().set_visible(False) plt.show()  fig = plt.imshow(pic_lowres_larran.astype(np.uint8), interpolation='nearest') fig.axes.get_xaxis().set_visible(False) fig.axes.get_yaxis().set_visible(False) plt.show()  (Optional) Taking snapshots from the webcam We are going to use the Python API of OpenCV in order to take pictures.\n# Define a function to take a snapshot def take_snapshot(camera_id=0, fallback_filename=None): camera = cv2.VideoCapture(camera_id) try: # Take 10 consecutive snapshots to let the camera automatically # tune itself and hope that the contrast and lightning of the # last snapshot is good enough. for i in range(10): snapshot_ok, image = camera.read() if snapshot_ok: image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) else: print('WARNING: Could not access the camera!') if fallback_filename: image = imread(fallback_filename) finally: camera.release() return image  # Test if the function take_snapshot is working. pic = take_snapshot(camera_id=0, fallback_filename='laptop.jpeg')  Image Classification The Keras library includes several neural network models pretrained on the ImageNet classification dataset. A popular model that show a good tradeoff between computation speed, model size and accuracy is called ResNet-50 ( here for the article on Deep Residual Networks).\n# Get the ResNet-50 model model = resnet50.ResNet50(weights='imagenet')  Let\u0026rsquo;s check that Tensorflow backend used by Keras as the default backend expect the color channel on the last axis. If it had not been the case, it would have been possible to change the order of the axes with pic = pic.transpose(2, 0, 1).\nThe color channel is on the channels_last. The network has been trained on (224, 224) RGB images.  None is used by Keras to mark dimensions with a dynamic number of elements. Here, None is the batch size, that is the number of images that can be processed at one. In the following, we will process only image at a time.\n# Load and resize the picture pic = imread('laptop.jpeg') pic_224 = resize(pic, (224, 224), preserve_range=True, mode='reflect')  The shape of the picture is (224, 224, 3), and the dtype of its elements is float64.  However, the model use float32 dtype. So, we have to convert the picture into float32.\npic_224 = pic_224.astype(np.float32)  fig = plt.imshow(pic_224 / 255, interpolation='nearest') fig.axes.get_xaxis().set_visible(False) fig.axes.get_yaxis().set_visible(False) plt.show()  Note that the image has been deformed by the resizing. In practice, this should not degrade the performance of the network too much. There are two alternatives solutions to that problems:\n  resizing the image so that the smallest side is set to 224;\n  extracting a square centered crop of size $(224, 224)$ from the resulting image.\nThe shape of the picture is (224, 224, 3), whereas the input shape of the model should be (None, 224, 224, 3). So we have to expand the dimension of the picture.\n  pic_224_batch = pic_224[None, ...] # or np.expand_dims(pic_224, axis=0)  pic_224_batch is now compatible with the input shape of the neural network, so let\u0026rsquo;s make a prediction.\n%%time X = resnet50.preprocess_input(pic_224_batch.copy()) pred = model.predict(X)  CPU times: user 1.79 s, sys: 52 ms, total: 1.84 s Wall time: 1.49 s  Note that we make a copy each time as the function preprocess_input can modify the image inplace to reuse memory when preprocessing large datasets.\nThe output predictions are a 2D array with:\n One row per image in the batch; One column per target class in the ImageNet LSVRC dataset ($1000$ possible classes) with the probabilities that a given image belongs to a particular class. Obviously, the sum of the columns for each row is equal to $1$.  Decoding the predicition probabilities Reading the raw probabilities for the $1000$ possible ImageNet classes is tedious. Fortunately, Keras comes with an helper function to extract the highest rated classes according to the model and display both class names and the wordnet synset identifiers.\nprint('Predicted image labels:') for class_id, class_name, confidence in resnet50.decode_predictions(pred, top=5)[0]: print(f\u0026quot;\\t* {class_name} (synset: {class_id}): {confidence}\u0026quot;)  Predicted image labels: * notebook (synset: n03832673): 0.3599511384963989 * laptop (synset: n03642806): 0.25687211751937866 * desk (synset: n03179701): 0.15139059722423553 * mouse (synset: n03793489): 0.11147501319646835 * desktop_computer (synset: n03180011): 0.051331911236047745  Check on the ImageNet website to better understand the use of the terms notebook in the training set. Note that the network is not too confident about the class of the main object in that image. If we were to merge the notebook and the laptop classes, the prediction will be good.\nFurthermore, the network also considers secondary objects (desk, mouse, \u0026hellip;) but the model as been trained as an image (multiclass) classification model with a single expected class per image rather than a multi-label classification model such as an object detection model with several positive labels per image.\nWe have to keep that in mind when trying to make use of the predictions of such a model for a practical application. This is a fundamental limitation of the label structure of the training set.\nA note on preprocessing All Keras pretrained vision models expect images with float32 dtype and values in the $[0, 255]$ range. When training neural network, it often works better to have values closer to zero.\n A typical preprocessing is to center each of the channel and normalize its variance. Another one is to measure the min and the max values and to shift and rescale to the $(-1, 1)$ range.  The exact kind of preprocessing is not very important, but it\u0026rsquo;s very important to always reuse the preprocessing function that was used when training the model.\nNow, we are going to use different model, ResNet50, MobileNet and InceptionResNetV2, to classify the images from Wikipedia. We can then compare the models in both time and prediction accuracy.\nWe start be defining some function to predict the object in the images.\ndef classify_resnet50(model, fallback_filename=None): \u0026quot;\u0026quot;\u0026quot; Function that takes a snapshot of the webcam and display it along with the decoded prediction of the model and their confidence level. \u0026quot;\u0026quot;\u0026quot; # Take a snapshot pic = take_snapshot(camera_id=0, fallback_filename=fallback_filename) # Preprocess the picture pic_224 = resize(pic, (224, 224), preserve_range=True, mode='reflect') pic_224_batch = pic_224[None, ...] # Do predictions pred = model.predict(resnet50.preprocess_input(pic_224_batch.copy())) # Show the pic fig = plt.imshow(pic / 255, interpolation='nearest') fig.axes.get_xaxis().set_visible(False) fig.axes.get_yaxis().set_visible(False) plt.show() # Print the decoded predictions print('Predicted image labels:') for class_id, class_name, confidence in resnet50.decode_predictions(pred, top=5)[0]: print(f\u0026quot;\\t* {class_name} (synset: {class_id}): {confidence}\u0026quot;) def classify_mobilenet(model, fallback_filename=None): \u0026quot;\u0026quot;\u0026quot; Function that takes a snapshot of the webcam and display it along with the decoded prediction of the model and their confidence level. \u0026quot;\u0026quot;\u0026quot; # Take a snapshot pic = take_snapshot(camera_id=0, fallback_filename=fallback_filename) # Preprocess the picture pic_224 = resize(pic, (224, 224), preserve_range=True, mode='reflect') pic_224_batch = pic_224[None, ...] # Do predictions pred = model.predict(mobilenet.preprocess_input(pic_224_batch.copy())) # Show the pic fig = plt.imshow(pic / 255, interpolation='nearest') fig.axes.get_xaxis().set_visible(False) fig.axes.get_yaxis().set_visible(False) plt.show() # Print the decoded predictions print('Predicted image labels:') for class_id, class_name, confidence in mobilenet.decode_predictions(pred, top=5)[0]: print(f\u0026quot;\\t* {class_name} (synset: {class_id}): {confidence}\u0026quot;) def classify_inception_resnet_v2(model, fallback_filename=None): \u0026quot;\u0026quot;\u0026quot; Function that takes a snapshot of the webcam and display it along with the decoded prediction of the model and their confidence level. \u0026quot;\u0026quot;\u0026quot; # Take a snapshot pic = take_snapshot(camera_id=0, fallback_filename=fallback_filename) # Preprocess the picture pic_299 = resize(pic, (299, 299), preserve_range=True, mode='reflect') pic_299_batch = pic_299[None, ...] # Do predictions pred = model.predict(inception_resnet_v2.preprocess_input(pic_299_batch.copy())) # Show the pic fig = plt.imshow(pic / 255, interpolation='nearest') fig.axes.get_xaxis().set_visible(False) fig.axes.get_yaxis().set_visible(False) plt.show() # Print the decoded predictions print('Predicted image labels:') for class_id, class_name, confidence in inception_resnet_v2.decode_predictions(pred, top=5)[0]: print(f\u0026quot;\\t* {class_name} (synset: {class_id}): {confidence}\u0026quot;)  %%bash wget https://upload.wikimedia.org/wikipedia/commons/3/3f/JPEG_example_flower.jpg wget https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Alcatel_one_touch_easy.jpg/450px-Alcatel_one_touch_easy.jpg wget https://upload.wikimedia.org/wikipedia/commons/thumb/8/8d/President_Barack_Obama.jpg/480px-President_Barack_Obama.jpg  We will begin the models comparison using the model ResNet50.\n# Get the ResNet50 model model_resnet50 = resnet50.ResNet50(weights='imagenet')  %%time classify_resnet50(model, 'JPEG_example_flower.jpg')  Predicted image labels: * pot (synset: n03991062): 0.37311553955078125 * hair_slide (synset: n03476684): 0.08061367273330688 * strawberry (synset: n07745940): 0.06821887940168381 * vase (synset: n04522168): 0.05385832488536835 * ant (synset: n02219486): 0.044536933302879333 CPU times: user 740 ms, sys: 112 ms, total: 852 ms Wall time: 326 ms  %%time classify_resnet50(model, '450px-Alcatel_one_touch_easy.jpg')  Predicted image labels: * cellular_telephone (synset: n02992529): 0.9453017711639404 * radio (synset: n04041544): 0.022191418334841728 * hand-held_computer (synset: n03485407): 0.02068745717406273 * combination_lock (synset: n03075370): 0.0010540278162807226 * pay-phone (synset: n03902125): 0.001006232458166778 CPU times: user 716 ms, sys: 48 ms, total: 764 ms Wall time: 307 ms  %%time classify_resnet50(model, '480px-President_Barack_Obama.jpg')  Predicted image labels: * groom (synset: n10148035): 0.7057098746299744 * suit (synset: n04350905): 0.2643233835697174 * gown (synset: n03450230): 0.009134724736213684 * Windsor_tie (synset: n04591157): 0.008021678775548935 * Loafer (synset: n03680355): 0.007970282807946205 CPU times: user 736 ms, sys: 40 ms, total: 776 ms Wall time: 310 ms  # Get the MobileNet model model_mobilenet = mobilenet.MobileNet(weights='imagenet')  %%time classify_mobilenet(model_mobilenet, 'JPEG_example_flower.jpg')  Predicted image labels: * lemon (synset: n07749582): 0.3409530222415924 * orange (synset: n07747607): 0.2852575182914734 * sulphur_butterfly (synset: n02281406): 0.1666731983423233 * strawberry (synset: n07745940): 0.09244516491889954 * whistle (synset: n04579432): 0.01351318508386612 CPU times: user 1.72 s, sys: 52 ms, total: 1.77 s Wall time: 1.56 s  %%time classify_mobilenet(model_mobilenet, '450px-Alcatel_one_touch_easy.jpg')  Predicted image labels: * cellular_telephone (synset: n02992529): 0.5218481421470642 * hand-held_computer (synset: n03485407): 0.4698248505592346 * radio (synset: n04041544): 0.004865806549787521 * dial_telephone (synset: n03187595): 0.0011262071784585714 * remote_control (synset: n04074963): 0.0005011822795495391 CPU times: user 420 ms, sys: 12 ms, total: 432 ms Wall time: 204 ms  %%time classify_mobilenet(model_mobilenet, '480px-President_Barack_Obama.jpg')  Predicted image labels: * groom (synset: n10148035): 0.6955855488777161 * Windsor_tie (synset: n04591157): 0.17594707012176514 * suit (synset: n04350905): 0.09634945541620255 * organ (synset: n03854065): 0.006460004951804876 * theater_curtain (synset: n04418357): 0.006234230939298868 CPU times: user 412 ms, sys: 40 ms, total: 452 ms Wall time: 225 ms  Now, let\u0026rsquo;s fit the InceptionResNetV2 model.\n# Get the InceptionResNetV2 model model_inception_resnet_v2 = inception_resnet_v2.InceptionResNetV2(weights='imagenet')  %%time classify_inception_resnet_v2(model_inception_resnet_v2, 'JPEG_example_flower.jpg')  Predicted image labels: * lemon (synset: n07749582): 0.659601628780365 * orange (synset: n07747607): 0.10027674585580826 * pot (synset: n03991062): 0.09215866029262543 * strawberry (synset: n07745940): 0.012752575799822807 * sulphur_butterfly (synset: n02281406): 0.010476206429302692 CPU times: user 7.52 s, sys: 120 ms, total: 7.64 s Wall time: 6.52 s  %%time classify_inception_resnet_v2(model_inception_resnet_v2, '450px-Alcatel_one_touch_easy.jpg')  Predicted image labels: * cellular_telephone (synset: n02992529): 0.8715960383415222 * hand-held_computer (synset: n03485407): 0.013546744361519814 * radio (synset: n04041544): 0.004702822770923376 * modem (synset: n03777754): 0.004240750335156918 * combination_lock (synset: n03075370): 0.0029042132664471865 CPU times: user 1.59 s, sys: 92 ms, total: 1.68 s Wall time: 574 ms  %%time classify_inception_resnet_v2(model_inception_resnet_v2, '480px-President_Barack_Obama.jpg')  Predicted image labels: * suit (synset: n04350905): 0.39843690395355225 * groom (synset: n10148035): 0.3309420049190521 * Windsor_tie (synset: n04591157): 0.06947924941778183 * theater_curtain (synset: n04418357): 0.04553558677434921 * bow_tie (synset: n02883205): 0.004989056382328272 CPU times: user 1.58 s, sys: 68 ms, total: 1.64 s Wall time: 574 ms  Instance Detection and Segmentation with Mask-RCNN  Mask-RCNN is a refinement of the Faster-RCNN object detection model to also add support for instance segmentation. The following shows how to use a Keras based implementation provided by Matterport along with model parameters pretrained on the COCO Object Detection dataset .\n# Download the model and the COCO trained weights URL = \u0026quot;https://github.com/ogrisel/Mask_RCNN/archive/master.zip\u0026quot; FOLDER = 'maskrcnn' FILENAME = 'Mask_RCNN-master.zip' if not os.path.exists(FOLDER): if not os.path.exists(FILENAME): tic = time() print(f'Downloading {URL} to {FILENAME} (can take a couple of minutes)...') urlretrieve(URL, FILENAME) print(f'Done in {time() - tic}') print(f'Extracting archive to {FOLDER}...') zipfile.ZipFile(FILENAME).extractall('.') os.rename('Mask_RCNN-master', FOLDER) COCO_MODEL_FILE = 'mask_rcnn_coco.h5' if not os.path.exists(COCO_MODEL_FILE): from maskrcnn import utils print('Pretrained model can take several minutes to download.') utils.download_trained_weights(COCO_MODEL_FILE)  Create Model and Load Training Weights from maskrcnn import config from maskrcnn import model as modellib class InferenceCocoConfig(config.Config): # Give the configuration a recognizable name NAME = 'inference_coco' # Number of classes (including background) NUM_CLASSES = 1 + 80 # COCO has 80 classes. # Set batch size to 1 since we'll be running inference on # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU GPU_COUNT = 1 IMAGES_PER_GPU = 1 config = InferenceCocoConfig() model = modellib.MaskRCNN(mode='inference', model_dir='mask_rcnn/logs', config=config) # Load weights trained on MS-COCO COCO_MODEL_FILE = 'mask_rcnn_coco.h5' model.load_weights(COCO_MODEL_FILE, by_name=True)  Class Names Index of the class in the list is its ID. For example, to get ID of the teddy bear class, use: class_names.index('teddy bear'). BG stands for background.\n# COCO class names class_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']  Run Object Detection Let\u0026rsquo;s perform object segmentation on an image taken on the web.\n%%bash wget https://storage.needpix.com/rsynced_images/street-scene-2301158_1280.jpg  # Read the image pic = imread('street-scene-2301158_1280.jpg')  fig = plt.imshow(pic, interpolation='nearest') fig.axes.get_xaxis().set_visible(False) fig.axes.get_yaxis().set_visible(False) plt.show()  from maskrcnn import visualize # Run detection tic = time() results = model.detect([pic], verbose=1) toc = time() print(f'Image analyzed in {np.around(toc - tic, 2)} secondes.')  Processing 1 images image shape: (960, 1280, 3) min: 0.00000 max: 255.00000 molded_images shape: (1, 1024, 1024, 3) min: -123.70000 max: 150.11562 image_metas shape: (1, 89) min: 0.00000 max: 1280.00000 Image analyzed in 13.57 secondes.  # Visualize the results r = results[0] # Take the results for the first image. for class_id, score in zip(r['class_ids'], r['scores']): print(f'{class_names[class_id]}:\\t{score}')  car:\t0.9879944324493408 car:\t0.9879814982414246 car:\t0.9828053116798401 car:\t0.9798809289932251 car:\t0.9787180423736572 car:\t0.9613178968429565 car:\t0.9486448168754578 car:\t0.9470494389533997 motorcycle:\t0.9209350943565369 motorcycle:\t0.9195521473884583 car:\t0.9188504815101624 car:\t0.9100474119186401 car:\t0.9096055626869202 car:\t0.8942683339118958 car:\t0.8911943435668945 person:\t0.8910031914710999 person:\t0.8883078098297119 bus:\t0.8868105411529541 car:\t0.8834719061851501 person:\t0.8810924291610718 car:\t0.8774976134300232 car:\t0.870510995388031 bus:\t0.8356407284736633 motorcycle:\t0.8196616172790527 bus:\t0.8100437521934509 person:\t0.8088014721870422 person:\t0.8027144074440002 person:\t0.8002358675003052 car:\t0.7951725721359253 car:\t0.7892614006996155 person:\t0.787919282913208 motorcycle:\t0.7864757776260376 car:\t0.7821618318557739 person:\t0.771487832069397 person:\t0.7644047141075134 bus:\t0.7624491453170776 person:\t0.7461680769920349 car:\t0.7306007742881775 car:\t0.7271723747253418 car:\t0.7174354195594788 car:\t0.7103606462478638  # Visualization on the picture visualize.display_instances(pic, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'])  ","date":1565740800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565740800,"objectID":"5a4ce1d58920c3c285d830a4fc6e6929","permalink":"/post/introduction-to-pretrained-models-for-computer-vision/","publishdate":"2019-08-14T00:00:00Z","relpermalink":"/post/introduction-to-pretrained-models-for-computer-vision/","section":"post","summary":"In particular, we are going to use the ResNet50, MobileNet and InceptionResNetV2 models for Image Classification and Mask-RCNN models for Instance Detection and Segmentation.","tags":["Deep Learning","Python","Keras"],"title":"Introduction to Pretrained Models for Computer Vision","type":"post"},{"authors":null,"categories":["Deep Learning","Python"],"content":"This post is based on the Deep Learning course from the Master Datascience Paris Saclay. Materials of the course can be found here . The complete code can be found on a Kaggle kernel .\nGoal of the post\n Train a simple neural network (Multi-Layer Perceptron) with the high level framework Keras.  Dataset used\n The MNIST dataset ( Kaggle link ).   Figure 1: An example of an image in the dataset.  Preprocessing  During this step, we will do some normalization on both the training and testing dataset.  # Extract and convert the pixel as numpy array with dtype='float32' train = np.asarray(digits_train.iloc[:, 1:], dtype='float32') test = np.asarray(digits_test, dtype='float32') train_target = np.asarray(digits_train.loc[:, 'label'], dtype='int32')  # Scale the data scaler = preprocessing.StandardScaler() train_scale = scaler.fit_transform(train) test_scale = scaler.transform(test)   Figure 2: An example of a scaled image in the dataset.  A Feed Forward Neural Network with Keras Objectives  Build and train a first feedforward network using Keras. Guide to Sequential model in Keras. Experiment with different optimizers, activations, size of layers or initializations.  How Keras works?  In order to build a neural network, we need to turn the target variable into a \u0026ldquo;one-hot-encoding\u0026rdquo; vector representation. There are multiple to do so. One may use the function OneHotEncoder from sklearn.preprocessing, but as we want to work with Keras, we should use the utility function provided by him.  # Encoded the target vector as one-hot-encoding vector. target = to_categorical(train_target)    The high level API of Keras needs some objects to build a feed forward neural network:\n A model by stacking layers with the right dimensions; A loss function with an optimizer; Some training data.    Let\u0026rsquo;s build a first model with only one hidden layer with a tanh activation function.\n# Define some parameters N = train.shape[1] # Length of one data H = 100 # Dimension of the hidden layer K = 10 # Dimension of the output layer (number of classes to predict) lr = 0.1 # Learning rate for the loss function epochs = 15 # Number of epochs for the NN batch_size = 32 # Size of the batch # Define the model model = Sequential() model.add(Dense(H, input_dim=N, activation='tanh')) model.add(Dense(K, activation='softmax')) # Print the model model.summary()  _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_1 (Dense) (None, 100) 78500 _________________________________________________________________ dense_2 (Dense) (None, 10) 1010 ================================================================= Total params: 79,510 Trainable params: 79,510 Non-trainable params: 0 _________________________________________________________________  # Define the loss function with the SGD optimizer model.compile(optimizer=optimizers.SGD(lr=lr), loss='categorical_crossentropy', metrics=['accuracy']) # Fit the model history = model.fit(train_scale, target, epochs=epochs, batch_size=batch_size, verbose=0)   Figure 3: Accuracy and loss of the model through the epochs using the SGD optimizer.  Influence of the learning rate  Let\u0026rsquo;s look at the influence of the learning rate on the training loss and accuracy. We consider a learning rate from 0.001 to 10.  lrs = np.logspace(-3, 1, num=5) history = dict() for lr in lrs : model = Sequential() model.add(Dense(H, input_dim=N, activation='tanh')) model.add(Dense(K, activation='softmax')) model.compile(optimizer=optimizers.SGD(lr=lr), loss='categorical_crossentropy', metrics=['accuracy']) history[lr] = model.fit(train_scale, target, epochs=epochs, batch_size=batch_size, verbose=0).history   Figure 4: Accuracy and loss of the model through the epochs for different learning rates.  As we see on the graphs, the learning rate has an influence of the speed of convergence and even on the possible divergence. So, for this dataset, when the learning rate is 10, the model does not converge. However, for the learning rate equals to 0.001, 0.01 and 1, the model still converges but it is slower than for the learning rate of 0.1.\n Let\u0026rsquo;s modify the SGD optimizer to enable a Nesterov momentum of 0.9. The momentum is used to mitigate the small learning rate or slow training problem a little. However, here, it seems that is not working.  model = Sequential() model.add(Dense(H, input_dim=N, activation='tanh')) model.add(Dense(K, activation='softmax')) model.compile(optimizer=optimizers.SGD(lr=0.1, momentum=0.9, nesterov=True), loss='categorical_crossentropy', metrics=['accuracy']) history = model.fit(train_scale, target, epochs=epochs, batch_size=batch_size, verbose=0)   Figure 5: Accuracy and loss of the model through the epochs using the Nesterov momentum.  Influence of the optimizer  Now, let\u0026rsquo;s try the Adam optimizer.  model = Sequential() model.add(Dense(H, input_dim=N, activation='tanh')) model.add(Dense(K, activation='softmax')) model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy']) history = model.fit(train_scale, target, epochs=epochs, batch_size=batch_size, verbose=0)   Figure 6: Accuracy and loss of the model through the epochs using the Adam optimizer.  The Adam optimizer with default parameters performs quite well on this model. In fact, the results are comparable with the SGD optimizer with learning rate 0.1 whereas the default learning rate of the Adam optimizer is 0.001.\n Let\u0026rsquo;s add another hidden layer to the model with the relu activation function.  model = Sequential() model.add(Dense(H, input_dim=N, activation='relu')) model.add(Dense(H, activation='relu')) model.add(Dense(K, activation='softmax')) model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy']) history = model.fit(train_scale, target, epochs=epochs, batch_size=batch_size, verbose=0)   Figure 7: Accuracy and loss of the model through the epochs with another hidden layers.  When we add another hidden layer and change the activation function, the results with default parameters are in some sense less good than before. However, it does seem to be a problem to train the model with Adam default parameters.\n Let\u0026rsquo;s try a last one optimizer, the Adadelta optimizer (no learning rate to set).  model = Sequential() model.add(Dense(H, input_dim=N, activation='tanh')) model.add(Dense(K, activation='softmax')) model.compile(optimizer=optimizers.Adadelta(), loss='categorical_crossentropy', metrics=['accuracy']) history = model.fit(train_scale, target, epochs=epochs, batch_size=batch_size, verbose=0)   Figure 8: Accuracy and loss of the model through the epochs using the Adadelta optimizer.  The Adadelta seems to give very good results on this dataset. No learning rate are required for this optimizer, in fact, it adapts the learning rate over time.\nForward pass and generalization  Now, we are gonna use the last model we fit to make prediction on the test set.  prediction = model.predict_classes(test)   Figure 9: Examples of prediction.  Submitting this very simple model, with only one hidden layer, leads to a 88% accuracy.\nImpact of initialization Let us now study the impact of initialization when training a deep feed forward network. By default, Keras dense layers use the Glorot Uniform initialization strategy to initialize the weigth matrices:\n each weight coefficient is randomly sampled from $[-scale, scale]$; scale is proportional to $1 / \\sqrt{n_{in} + n_{out}}$.  This strategy is known to work well to initialize deep neural networks with tanh or relu activation functions and then trained with standard SGD. To assess the impact of initialization, let us plug an alternative init scheme into a two hidden layers networks with tanh activation functions. For the sake of the example, let\u0026rsquo;s use normal distributed weights with a manually adjustable scale (standard deviation) and see the impact of the scale value.\n# Define a random normal initializers. normal_init = initializers.RandomNormal(stddev=0.01) model = Sequential() model.add(Dense(H, input_dim=N, activation='tanh', kernel_initializer=normal_init)) model.add(Dense(K, activation='tanh', kernel_initializer=normal_init)) model.add(Dense(K, activation='softmax', kernel_initializer=normal_init)) model.compile(optimizer=optimizers.SGD(lr=lr), loss='categorical_crossentropy', metrics=['accuracy']) model.summary()  _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_22 (Dense) (None, 100) 78500 _________________________________________________________________ dense_23 (Dense) (None, 10) 1010 _________________________________________________________________ dense_24 (Dense) (None, 10) 110 ================================================================= Total params: 79,620 Trainable params: 79,620 Non-trainable params: 0 _________________________________________________________________  Let\u0026rsquo;s have a look at the parameters of the first layer after initialization but before any training has happened.\nw = model.layers[0].weights[0].eval(keras.backend.get_session())  Initialization weights: - mean = 0.0 - standard deviation = 0.009999999776482582  b = model.layers[0].weights[1].eval(keras.backend.get_session())  Initialization bias: - mean = 0.0 - standard deviation = 0.0  history = model.fit(train_scale, target, epochs=epochs, batch_size=batch_size, verbose=0)   Figure 10: Accuracy and loss of the model through the epochs with a Normal initialization for the weights.  We see that with this initialization the SGD algorithm can not train the network in 15 epochs.\n# Define different initializations init_list = [ ('Glorot Uniform Init', 'glorot_uniform'), ('Small Scale Init', initializers.RandomNormal(stddev=1e-3)), ('Large Scale Init', initializers.RandomNormal(stddev=1)), ('Zero Weights Init', 'zero') ] optimizer_list = [ ('SGD', optimizers.SGD(lr=lr)), ('Adam', optimizers.Adam()), ('SGD + Nesterov momentum', optimizers.SGD(lr=lr, momentum=0.9, nesterov=True)) ] history = dict() for optimizer_name, optimizer in optimizer_list: for init_name, init in init_list: model = Sequential() model.add(Dense(H, input_dim=N, activation='tanh', kernel_initializer=init)) model.add(Dense(K, activation='tanh', kernel_initializer=init)) model.add(Dense(K, activation='softmax', kernel_initializer=init)) model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy']) history[(optimizer_name, init_name)] = model.fit(train_scale, target, epochs=epochs, batch_size=batch_size, verbose=0)  Initialization with zero weights  Figure 11: Accuracy and loss of the models through the epochs with zero weights initialization.  If the network is initialized to zero weights, the activations of the hidden layers are always set to zero, whatever the value of the inputs. The gradient is always zero for all training samples and no learning can happen with any gradient-based optimizer (SGD, Adam, \u0026hellip;): the loss stays constant.\nA network with null weigths has null gradients but this is not a local minimum (nor a local maximum): it is a saddle point at the center of a neighborhood with very low gradients.\nInitialization with small scale  Figure 12: Accuracy and loss of the model through the epochs with small scale initialization.  Therefore when the scale of a random initializations of the weights is too small, SGD has a hard time evading that area of low gradients. Adding momentum can help but especially for deep networks it can take many epochs to evade the area.\nInitialization with large scale  Figure 13: Accuracy and loss of the model through the epochs with large scale initialization.  Initializating the weights with large random values will make the ouput distribution (softmax) very peaky: the network is very \u0026ldquo;confident\u0026rdquo; of its predictions even if they are completely random. This leads to a very high initial loss value.\nThe softmax function does not saturate (bad classification always have a non-zero gradient). However, the intermediate tanh layers can saturate, therefore squashing the gradient of the loss with respect to the parameters of the first \u0026ldquo;Dense\u0026rdquo; layer and making the network train much slower.\nInitialization with Glorot Uniform  Figure 13: Accuracy and loss of the model through the epochs with Glorot Uniform initialization.  The Glorot uniform init uses a scale that depends on the dimensions of the weight matrix so has to preserve the average norm of activations and flowing gradients so as to make learning possible. Keras provides alternatives that can be better in some cases. Look at this paper for more information.\nAdam tends to be more robust when it comes to bad initialization thanks to its per-weight learning rate adjustments but still benefits from a good initialization.\nThings to remember if your network fails to learn at all (the loss stays at its inital value):\n ensure that the weights are properly initialized; inspect the per-layer gradient norms to help identify the bad layer; use Adam instead of SGD as your default go to initializer.  ","date":1564963200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564963200,"objectID":"3f86b53254cf02583bd870d986dfbfc8","permalink":"/post/introduction-to-deep-learning-with-keras/","publishdate":"2019-08-05T00:00:00Z","relpermalink":"/post/introduction-to-deep-learning-with-keras/","section":"post","summary":"An introduction to neural network with the high level framework `Keras` and the famous MNIST dataset.","tags":["Deep Learning","Python","Keras"],"title":"Introduction to Deep Learning with Keras","type":"post"},{"authors":null,"categories":["R"],"content":"The package knitr has been created by Yihui Xie. It is disponible on the CRAN . Check out the official website for more information. It is designed to build and generate nice report in R.\nQuestions How to render Latex formula in ggplot graphs in a report in html? For a report in pdf, it is quite easy to render Latex formula in a ggplot graph. Just set the dev variable to 'tikz' in a R chunk. However, this method produces a pdf of the picture, and some browsers seem to have some trouble to show pdf files. So, the idea is to convert the pdf of the picture into a png file. For that, the R chunk accept the option fig.process and we will modify it to solve our problem.\nfig.process \u0026lt;- function(x) { x \u0026lt;- paste0('./', x) if(stringr::str_detect(x, 'pdf')){ y \u0026lt;- stringr::str_replace(x, 'pdf', 'png') png::writePNG(pdftools::pdf_render_page(x), target = y, dpi = 300) return(y) } else { return(x) } }  The function fig.process takes a string as input (the path of the picture). If the picture is in pdf, it will convert it into png, otherwise, it will do nothing. And finally, the browser will render the picture correctly.\n","date":1564012800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564012800,"objectID":"6a2d512187b7da01f9cb0ab41cf38ed5","permalink":"/post/knitr/","publishdate":"2019-07-25T00:00:00Z","relpermalink":"/post/knitr/","section":"post","summary":"The package `knitr` is a **R** package for report generation.","tags":["R","Package"],"title":"knitr","type":"post"},{"authors":["Steven Golovkine","Nicolas Klutchnikoff","Valentin Patilea"],"categories":null,"content":"","date":1559559600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559559600,"objectID":"2ed15dc92ee5ac9f679431a57b7d6163","permalink":"/talk/sfds-2019/","publishdate":"2019-06-03T11:00:00Z","relpermalink":"/talk/sfds-2019/","section":"talk","summary":"Une étude sur l'estimation de la covariance des données fonctionnelles dans le cadre de bruit hétéroscédastique.","tags":[],"title":"Données fonctionnelles avec erreur hétéroscédastique","type":"talk"},{"authors":["Steven Golovkine"],"categories":null,"content":"","date":1552921200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552921200,"objectID":"a2c1831de728abcd894372c81db63360","permalink":"/talk/mascot_num_2019/","publishdate":"2019-03-18T15:00:00Z","relpermalink":"/talk/mascot_num_2019/","section":"talk","summary":"A poster presentation about making groups in driving data.","tags":[],"title":"Clustering multivariate functional data defined on random domains: an application to vehicle trajectories analysis","type":"talk"},{"authors":null,"categories":["Python","Machine Learning"],"content":"  Context Question Exploration of the data Subset features selection Model construction   This dataset is provided by sammy123 on Kaggle. My study and the complete code are on a Kaggle kernel.\nContext Lower back pain can be caused by a variety of problems with any parts of the complex, interconnected network of spinal muscles, nerves, bones, discs or tendons in the lumbar spines. Typical sources of low back pain include:\n The large nerve roots in the low back that go to the legs may be irritated. The smaller nerves that supply the low back may be irritated. The large paired lower back muscles (erector spinae) may be strained. The bones, ligaments or joints may be damaged. An intervertebral disc may be degenerating.  An irritation or problem with any of these structures can cause lower back pain and/or pain that radiates or is referred to other parts of the body. Many lower back problems can also cause back muscle spasms, which do not sound like much but can cause severe pain and disability.\nWhile lower back pain is extremely common, the symptoms and severity of lower back pain vary greatly. A simple lower back muscle strain might be excruciating enough to necessitate an emergency room visit, while a degenerating disc might cause only mild, intermittent discomfort.\n Question How identify an abnormal or normal person using collected physical spine details and data?\n# Import the data data = pd.read_csv(\u0026quot;Dataset_spine.csv\u0026quot;, decimal=\u0026#39;.\u0026#39;, sep=\u0026#39;,\u0026#39;, header=0) data = data.drop(\u0026#39;Unnamed: 13\u0026#39;, 1) data.columns = [\u0026#39;pelvic_incidence\u0026#39;, \u0026#39;pelvic_tilt\u0026#39;, \u0026#39;lumbar_lordosis_angle\u0026#39;, \u0026#39;sacral_slope\u0026#39;, \u0026#39;pelvic_radius\u0026#39;, \u0026#39;degree_spondylolisthesis\u0026#39;, \u0026#39;pelvic_slope\u0026#39;, \u0026#39;direct_tilt\u0026#39;, \u0026#39;thoracic_slope\u0026#39;, \u0026#39;cervical_tilt\u0026#39;, \u0026#39;sacrum_angle\u0026#39;, \u0026#39;scoliosis_slope\u0026#39;, \u0026#39;class\u0026#39;] data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Pelvic Incidence   Pelvic Tilt   Lumbar Lordosis Angle   Sacral Slope   Pelvic Radius   Degree Spondylolisthesis   Pelvic Slope   Direct Tilt   Thoracic Slope   Cervical Tilt   Sacrum Angle   Scoliosis Slope   Class       63.027818   22.552586   39.609117   40.475232   98.672917   -0.254400   0.744503   12.5661   14.5386   15.30468   -28.658501   43.5123   Abnormal     39.056951   10.060991   25.015378   28.995960   114.405425   4.564259   0.415186   12.8874   17.5323   16.78486   -25.530607   16.1102   Abnormal     68.832021   22.218482   50.092194   46.613539   105.985135   -3.530317   0.474889   26.8343   17.4861   16.65897   -29.031888   19.2221   Abnormal     69.297008   24.652878   44.311238   44.644130   101.868495   11.211523   0.369345   23.5603   12.7074   11.42447   -30.470246   18.8329   Abnormal     49.712859   9.652075   28.317406   40.060784   108.168725   7.918501   0.543360   35.4940   15.9546   8.87237   -16.378376   24.9171   Abnormal       Exploration of the data  Let’s check if there are some missing values in this dataset.  data.info()  \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 310 entries, 0 to 309 Data columns (total 13 columns): pelvic_incidence 310 non-null float64 pelvic_tilt 310 non-null float64 lumbar_lordosis_angle 310 non-null float64 sacral_slope 310 non-null float64 pelvic_radius 310 non-null float64 degree_spondylolisthesis 310 non-null float64 pelvic_slope 310 non-null float64 direct_tilt 310 non-null float64 thoracic_slope 310 non-null float64 cervical_tilt 310 non-null float64 sacrum_angle 310 non-null float64 scoliosis_slope 310 non-null float64 class 310 non-null object dtypes: float64(12), object(1) memory usage: 31.6+ KB  Compute some basic statistics about the data.  data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }      Pelvic Incidence   Pelvic Tilt   Lumbar Lordosis Angle   Sacral Slope   Pelvic Radius   Degree Spondylolisthesis   Pelvic Slope   Direct Tilt   Thoracic Slope   Cervical Tilt   Sacrum Angle   Scoliosis Slope       Count   310.000000   310.000000   310.000000   310.000000   310.000000   310.000000   310.000000   310.000000   310.000000   310.000000   310.000000   310.000000     Mean   60.496653   17.542822   51.930930   42.953831   117.920655   26.296694   0.472979   21.321526   13.064511   11.933317   -14.053139   25.645981     Std   17.236520   10.008330   18.554064   13.423102   13.317377   37.559027   0.285787   8.639423   3.399713   2.893265   12.225582   10.450558     Min   26.147921   -6.554948   14.000000   13.366931   70.082575   -11.058179   0.003220   7.027000   7.037800   7.030600   -35.287375   7.007900     25%   46.430294   10.667069   37.000000   33.347122   110.709196   1.603727   0.224367   13.054400   10.417800   9.541140   -24.289522   17.189075     50%   58.691038   16.357689   49.562398   42.404912   118.268178   11.767934   0.475989   21.907150   12.938450   11.953835   -14.622856   24.931950     75%   72.877696   22.120395   63.000000   52.695888   125.467674   41.287352   0.704846   28.954075   15.889525   14.371810   -3.497094   33.979600     Max   129.834041   49.431864   125.742385   121.429566   163.071041   418.543082   0.998827   36.743900   19.324000   16.821080   6.972071   44.341200      No results seem to be unusual, except for the maximum of the Degree Spondylolisthesis. Usually, a degree is between -180° and 180° (or 0° and 360°). If we look at the other data, it seems that the coding of the angle is between -180° and 180° (with a very few negative angle). Let’s look at all the values out of the usual range of the degrees (it concerns only the variable Degree Spondylolisthesis).\ndata[data.degree_spondylolisthesis \u0026gt; 180] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }      Pelvic Incidence   Pelvic Tilt   Lumbar Lordosis Angle   Sacral Slope   Pelvic Radius   Degree Spondylolisthesis   Pelvic Slope   Direct Tilt   Thoracic Slope   Cervical Tilt   Sacrum Angle   Scoliosis Slope   Class       115   129.834041   8.404475   48.384057   121.429566   107.690466   418.543082   0.860223   18.5943   11.1514   11.36543   -34.202073   27.5144   Abnormal      Only one observation has a Degree Spondylolisthesis larger than 180. We can consider a typo in the decimal of this value. So, we replace the value 418.543082 by 41.8543082.\ndata.loc[115, \u0026#39;degree_spondylolisthesis\u0026#39;] = 41.8543082  Recode the variable class into a dummy variable (0: Abnormal, 1: Normal).  data[\u0026#39;class\u0026#39;] = pd.get_dummies(data[\u0026#39;class\u0026#39;], prefix=\u0026#39;class\u0026#39;, drop_first=True)  Then, we look at the correlation between the different variables.  # Compute the correlation matrix. corr_data = round(data.corr(),2)   So, it appears that the class {Abnormal, Normal} is negatively correlated with the Pelvic Incidence, the Pelvic Tilt, the Lumbar Lordosis Angle, the Sacral Slope and the Degree Spondylolisthesis and positively correlated with the Pelvic Radius. The class has a very small correlation with the other variables.\n Let’s look at some boxplot for these variables.     Subset features selection model = ExtraTreesClassifier(n_estimators=200, random_state=0) model.fit(data.drop(\u0026#39;class\u0026#39;, axis=1, inplace=False), data[\u0026#39;class\u0026#39;]) importances = model.feature_importances_ importances_std = np.std([model_tree.feature_importances_ for model_tree in model.estimators_], axis=0)   So, we have an importance score for each attribute where the larger score the more important the attribute. As we see on the correlation plot, the variable degree spondylolisthesis and pelvic radius/pelvic tilt/pelvic incidence/lumbar lordosis angle are strongly correlated. We will consider only the variables Degree Spondylolisthesis, Pelvic Radius, Pelvic Tilt and Pelvic Incidence for building the model (the four with the strongest importance).\n Let’s plot these variables with the class.     Model construction  Split the dataset into train and test set.  X_train, X_test, Y_train, Y_test = train_test_split(data[[\u0026#39;degree_spondylolisthesis\u0026#39;, \u0026#39;pelvic_radius\u0026#39;, \u0026#39;pelvic_tilt\u0026#39;, \u0026#39;pelvic_incidence\u0026#39;]], data[\u0026#39;class\u0026#39;], test_size=1/3, random_state=42) scaler = StandardScaler().fit(X_train) X_train_transformed = scaler.transform(X_train) X_test_transformed = scaler.transform(X_test)  Let’s construct the baseline by setting the most frequent response in the training set to compare our model.  dummy = DummyClassifier(strategy=\u0026#39;most_frequent\u0026#39;, random_state=42) dummy.fit(X_train_transformed, Y_train) Y_pred_dummy = dummy.predict(X_test_transformed) Y_pred_proba_dummy = dummy.predict_proba(X_test_transformed)[:, 1] [fpr_dummy, tpr_dummy, thr_dummy] = metrics.roc_curve(Y_test, Y_pred_proba_dummy)  The accuracy for the dummy classifier is 72%.\n  Use the Logistic Regression method to predict the class (by Cross-Validation and GridSearch).  param_log_reg = {\u0026#39;tol\u0026#39;: np.logspace(-5, 1, 7), \u0026#39;C\u0026#39;: np.logspace(-3, 3, 7), \u0026#39;penalty\u0026#39;: [\u0026#39;l2\u0026#39;]} log_reg = GridSearchCV(LogisticRegression(solver=\u0026#39;lbfgs\u0026#39;), param_log_reg, cv=10, iid=False) log_reg.fit(X_train_transformed, Y_train)  Best parameters set found on development set: {‘C’: 1.0, ‘penalty’: ‘l2’, ‘tol’: 1.0}\n Y_pred_log_reg = log_reg.predict(X_test_transformed) Y_pred_proba_log_reg = log_reg.predict_proba(X_test_transformed)[:, 1] [fpr_log_reg, tpr_log_reg, thr_log_reg] = metrics.roc_curve(Y_test, Y_pred_proba_log_reg)  The accuracy for the Logistic Regression classifier is 85%.\n  Plot the ROC curves for each models     ","date":1546646400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546646400,"objectID":"f1b4374ef2e0c8b554a24de91f4809a0","permalink":"/project/lower-back-pain/","publishdate":"2019-01-05T00:00:00Z","relpermalink":"/project/lower-back-pain/","section":"project","summary":"Lower back pain can be caused by a variety of problems. How identify an abnormal or normal person using collected physical spine details and data?","tags":["Python","Machine Learning","Logistic Regression"],"title":"Lower Back Pain","type":"project"},{"authors":null,"categories":["R"],"content":"The package ggplot2 has been created by Hadley Wickham among others. It is disponible on the CRAN . Check out the official website for more information. It is designed to provide nice graphics in an quite easy way.\nUse custom theme First, define a particular function that define a new theme based on another.\ntheme_custom = function(base_family = \u0026quot;Times\u0026quot;){ theme_minimal(base_family = base_family) %+replace% theme( plot.title = element_text(size = 20), plot.subtitle = element_text(size = 16, vjust = -1), axis.title = element_text(size = 18), axis.text = element_text(size = 16), axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0), angle = 90), axis.title.x = element_text(margin = margin(t = 20, r = 0, b = 20, l = 0)), strip.text.x = element_text(size = 16), strip.text.y = element_text(size = 16), legend.text = element_text(size = 18), legend.text.align = 0 ) }  Then, just use it as any other theme.\nggplot(aes(x, y)) + theme_custom()  ","date":1545177600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545177600,"objectID":"9d69d4fcc8064ef19ef1add78fa7f128","permalink":"/post/package-ggplot2/","publishdate":"2018-12-19T00:00:00Z","relpermalink":"/post/package-ggplot2/","section":"post","summary":"The package `ggplot2` is a **R** package for data vizualisation. It is part of the tidyverse.","tags":["R","Package"],"title":"Package ggplot2","type":"post"},{"authors":null,"categories":["Hive"],"content":"Set up on MacOs Let\u0026rsquo;s install Hive with Homebrew.\nbrew install hive  The Hive\u0026lsquo;s files are in the folder /usr/local/Cellar/hive/*.\nSame as Hadoop, add some environment variables to the .bashrcfile.\n# Hive environment export HIVE_HOME=/usr/local/Cellar/hive/*/libexec PATH=$HIVE_HOME/bin:$PATH  In order to launch Hive, Hadoop ressources must be set up.\n# Setup Hadoop $HADOOP_HOME/sbin/start-dfs.sh $HADOOP_HOME/sbin/start-yarn.sh # Launch Hive hive  Common errors Metastore troubles Just remove and reload the metastore.\nrm -rf metastore_db derby.log $HIVE_HOME/bin/schematool -initSchema -dbType derby  ","date":1544313600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544313600,"objectID":"71c762579368d48148b4c2331e21a925","permalink":"/post/set-up-hive/","publishdate":"2018-12-09T00:00:00Z","relpermalink":"/post/set-up-hive/","section":"post","summary":"Apache Hive is a data warehouse software that facilitates the manipulation of large distributed datasets using SQL.","tags":["Hive","Set up"],"title":"Set up Hive","type":"post"},{"authors":null,"categories":["Spark"],"content":"Set up on MacOs Let\u0026rsquo;s install Spark with Homebrew.\nbrew install apache-spark  The Spark\u0026lsquo;s files are in the folder /usr/local/Cellar/apache-spark/*.\nSame as Hadoop, add some environment variables to the .bashrcfile.\n# Spark environment export SPARK_HOME=/usr/local/Cellar/apache-spark/*/libexec PATH=$SPARK_HOME/bin:$PATH  Configure Spark to run with YARN Edit the file $SPARK_HOME/conf/spark-env.sh.template by adding the following line:\nHADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop  And then rename it as spark-env.sh.\nFinally, you can run the Spark command lines on YARN with the command:\nspark-shell --master yarn --deploy-mode client  ","date":1544313600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544313600,"objectID":"35a2eac4b02301b8d8d07ca40d8e8d73","permalink":"/post/set-up-spark/","publishdate":"2018-12-09T00:00:00Z","relpermalink":"/post/set-up-spark/","section":"post","summary":"Apache Spark is a software for large-scale data processing.","tags":["Spark","Set up"],"title":"Set up Spark","type":"post"},{"authors":null,"categories":["Hadoop"],"content":"Hadoop deployment modes There are three ways to deploy Hadoop:\n Local mode Pseudo-distributed mode Distributed mode  Requisites to the installation Java Check if Java is installed:\njava -version  It should return something like that:\njava version \u0026quot;1.8.0_***\u0026quot; Java(TM) SE Runtime Environment (build 1.8.0_***-b11) Java HotSpot(TM) 64-Bit Server VM (build 25.***-b11, mixed mode)  If not, you could go on java.com and download it.\nSSH On MacOS, the Remote Login must be enable to authorise SSH. It is located in Systeme Preference and Sharing.\nTry to ssh to localhost without a passphrase/password. This is important because we do not want to enter a passphrase/password every time Hadoop connect to a node.\nssh localhost  If you can not, run these commands to create a key and put it into the authorised one to connect.\nssh-keygen -t rsa -P \u0026quot;\u0026quot; cat ~/.ssh/id_rsa.pub \u0026gt;\u0026gt; ~/.ssh/authorized_keys  Set up on MacOs Let\u0026rsquo;s install Hadoop with Homebrew.\nbrew install hadoop  As a result, we see where are Hadoop\u0026rsquo;s config files:\n/usr/local/opt/hadoop/libexec/etc/hadoop/hadoop-env.sh /usr/local/opt/hadoop/libexec/etc/hadoop/mapred-env.sh /usr/local/opt/hadoop/libexec/etc/hadoop/yarn-env.sh  Moreover, the JAVA_HOME has been set to the result of the command /usr/libexec/java_home. And finally, the Hadoop\u0026rsquo;s files are in the folder /usr/local/Cellar/hadoop/*. Now, in order to simplify the commands, it is common to add some environment variables to the .bashrc file.\n# Hadoop environment export HADOOP_HOME=/usr/local/Cellar/hadoop/*/libexec PATH=$HADOOP_HOME/bin:$PATH  Configure HDFS for the Pseudo-Distributed mode Use a single DataNode for each block Add the following lines to the file $HADOOP_HOME/etc/hadoop/hdfs-site.xml.\n\u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.replication\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;1\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt;  Configure the NameNode port Add the following lines to the file $HADOOP_HOME/etc/hadoop/core-site.xml.\n\u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;fs.defaultFS\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://localhost:9000\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt;  Set the runtime framework for executing MapReduce jobs Add the following lines to the file $HADOOP_HOME/etc/hadoop/mapred-site.xml.\n\u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapreduce.framework.name\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;yarn\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt;  Implement the service mapreduce_shuffle. Add the following lines to the file $HADOOP_HOME/etc/hadoop/yarn-site.xml.\n\u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.aux-services\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;mapreduce_shuffle\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt;  Format the filesystem hdfs namenode -format  Then, you can start the NameNode and DataNode deamons.\n$HADOOP_HOME/sbin/start-dfs.sh  It is possible to check if it\u0026rsquo;s working using the UI interface: http://localhost:50070/ or http://localhost:9870/ .\nAnd start the Ressource and Node managers.\n$HADOOP_HOME/sbin/start-yarn.sh  It is possible to check if it\u0026rsquo;s working using the UI interface: http://localhost:8088/ .\nCommon Errors Incompatible clusterIDs You should reformat the name node with the right clusterId.\nhdfs namenode -format -clusterId CID-...  ","date":1544227200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544227200,"objectID":"2df04cf4e74ca299319c06226add4f40","permalink":"/post/hadoop/","publishdate":"2018-12-08T00:00:00Z","relpermalink":"/post/hadoop/","section":"post","summary":"Apache Hadoop is an open-source project for reliable, scalable and distributed computing.","tags":["Hadoop","Set up"],"title":"Set up Hadoop","type":"post"},{"authors":null,"categories":["Python"],"content":"Update IRkernel after updating R Modify the file kernel.json into the folder ~/Library/Jupyter/kernels/ir. Replace the value of the argument argv by the new path of R.\n","date":1544140800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544140800,"objectID":"0454671bae3d066d5539f77b16433be9","permalink":"/post/jupyter-notebook/","publishdate":"2018-12-07T00:00:00Z","relpermalink":"/post/jupyter-notebook/","section":"post","summary":"Jupyter is a programming software which use notebooks widely used in data science.","tags":["Python","Set up"],"title":"Jupyter Notebook","type":"post"},{"authors":null,"categories":["Software"],"content":" Homebrew names itself as the missing package manager for macOS. It simplifies the installation and the management of the different softwares you could have.\nInstallation First, you need to have the Command Line Tools for Xcode. The installation of Xcode is made from the App Store. Once it is done, you can install the Command Line Tools using the following command in the terminal:\nxcode-select --install  Then, you should launch the following command to have Homebrew installed:\nruby -e \u0026quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\u0026quot;  After, you should tell to the system to take into consideration programs installed by Homebrew rather than the system default. By default, Homebrew uses the /usr/local/bin path. We do this by the adding this path to the $PATH environment variable.\necho 'export PATH=\u0026quot;/usr/local/bin/:$PATH\u0026quot;' \u0026gt;\u0026gt; ~/.bash_profile  Usage We install and uninstall a formula by using the commands:\nbrew install \u0026lt;formula\u0026gt; brew uninstall \u0026lt;formula\u0026gt;  To upgrade all the formulae, run:\nbrew update brew upgrade  To list all the formulae you have with their version, run:\nbrew list --versions  Finally, Homebrew keeps a trace of the previous versions of each of the formula (if you want to get it back). If you want to delete it, run:\nbrew cleanup  Cask  Homebrew-Cask extends Homebrew and allows to install software using command-line tools.\nTo look for a software, run:\nbrew cask search \u0026lt;formula\u0026gt;  To install/uninstall a software, run:\nbrew cask install \u0026lt;formula\u0026gt; brew cask uninstall \u0026lt;formula\u0026gt;  To know the outdated formulae, run:\nbrew cask outdated  And then, for update the package, run:\nbrew cask reinstall \u0026lt;formula\u0026gt;  ","date":1544054400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544054400,"objectID":"6e590359498c7e45ba73a9346b04e67a","permalink":"/post/homebrew/","publishdate":"2018-12-06T00:00:00Z","relpermalink":"/post/homebrew/","section":"post","summary":"Hombrew is a package manager for macOS. This software is a must to have for macOS users.","tags":["Software","Set up"],"title":"Homebrew","type":"post"},{"authors":null,"categories":["Python"],"content":" Matplotlib is a Python 2D plotting library.\nUse another font First, you need to download/get back the font in the ttf format. Once it is done, you have to find out the location of the matplotlib library.\npython -c \u0026quot;import matplotlib; print(matplotlib.matplotlib_fname())\u0026quot;  This folder will be different for everyone depending on the installation but it should ended by /matplotlib/mpl-data/matplotlibrc.\nThen, you copy the font into the folder /matplotlib/mpl-data/fonts/ttf/.\ncp font.ttf ./matplotlib/mpl-data/fonts/ttf/  Finally, remove the font cache:\nrm ~/.matplotlib/fontList.py3k.cache  Now, you can use the new font:\nimport matplotlib.pyplot as plt plt.rc('font', family='font_name')  ","date":1544054400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544054400,"objectID":"73b2908d8f9ee53186167abb9f68b547","permalink":"/post/library-matplotlib/","publishdate":"2018-12-06T00:00:00Z","relpermalink":"/post/library-matplotlib/","section":"post","summary":"Matplotlib is a Python 2D plotting library.","tags":["Python","Library"],"title":"Library Matplotlib","type":"post"},{"authors":null,"categories":["Software"],"content":" MySQL is an open-source relational database management system.\nInstallation and configurations We simply use Homebrew to install MySQL under MacOS.\nbrew install mysql  Then, we can launch MySQL by running the command:\nbrew services start mysql # ==\u0026gt; Successfully started 'mysql' (label: homebrew.mxcl.mysql)  One can recommended to set a password for the root user and only authorize the access from localhost.\nmysql_secure_installation  And then, we re-launch MySQL.\nbrew services restart mysql  Connection We connect to MySQL by running the commands:\nmysql --host=localhost --user=root -p # Enter password : ****  It is not recommended to work in root on databases because this user has all privileges. One could create a restrictive user of the database.\nGRANT ALL PRIVILEGES ON nom_base.* TO 'name'@'localhost' IDENTIFIED BY 'password';  One can save the database into a file using the command:\nmysqldump -u user -p --opt database_name \u0026gt; save.sql  Configuration In order to use every possible characters into the string in the database, one should activate the UTF-8 encode.\nmysql --host=localhost --user=name -p --default_character-set=utf8 # Enter password : ****  In order to insert data into the database using external files, one should activate this possibility.\nmysql -h localhost -u name -p --enable-local-infile # Enter password : ****  ","date":1544054400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544054400,"objectID":"25d9ba3d3ab6374026931b18d52f8cc3","permalink":"/post/mysql/","publishdate":"2018-12-06T00:00:00Z","relpermalink":"/post/mysql/","section":"post","summary":"MySQL is an open-source relational database management system.","tags":["Software","Set up"],"title":"MySQL","type":"post"},{"authors":null,"categories":["R"],"content":"The package xml2 has been created by Hadley Wickham among others. It is disponible on the CRAN . It is designed to work with XML files in R.\nInstallation issues Configuration failed because libxml-2.0 was not found First, check that the folders given by the following commands exits:\nwhich xml2-config # /usr/bin/xml2-config xml2-config --libs # -L/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.13.sdk/usr/lib -lxml2 -lz -lpthread -licucore -lm xml2-config --cflags # -I/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.13.sdk/usr/include/libxml2 which pkg-config # /usr/local/bin/pkg-config pkg-config --cflags libxml-2.0 # -I/usr/include/libxml2 pkg-config --libs libxml-2.0 # -lxml2  In the case where, /usr/include does not exists, run the command:\nxcode-select --install  And then, re-try to install the package xml2.\n","date":1544054400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544054400,"objectID":"bb657d2f529f779c6cfa085f34826840","permalink":"/post/package-xml2/","publishdate":"2018-12-06T00:00:00Z","relpermalink":"/post/package-xml2/","section":"post","summary":"The package `xml2` is a **R** package for parsing XML files.","tags":["R","Package"],"title":"Package xml2","type":"post"},{"authors":null,"categories":["Python"],"content":" Python is already installed on macOS. But as we do not want to mess with it, and have more flexibility, we will install our own version of Python. This post is based on this post by Henrique Bastos on Medium.\nWanted configurations  CPython 2.7 and CPython 3.7, but it is possible to install other implementations like PyPy or Anaconda. Python3 as a default version for everything, but it must easily change to Python2. A unique Jupyter Notebook/Lab working with both Python2 and Python3, and being able to detect the active virtual environment. A console iPython for Python3 and one for Python2, so no need to install it in every virtual environment of the projects. virtualenvwrapper to develop the different projects and change the context in one command.  Installation pyenv is probably the best way to install Python on macOS. Everything should be installed in the common directory without interfering with the rest of the system. Moreover, it handles with a lot of Python implementation such as CPython, PyPy, Anaconda, etc. And all of that with only one command. Firstly, one should install pyenv and two add-ons:\n pyenv to install Python implementations; pyenv-virtualenv to configure global environment; pyenv-virtualenvwrapper to work on projects.  brew install pyenv brew install pyenv-virtualenv brew install pyenv-virtualenvwrapper  With virtualenvwrapper, every virtualenv will be kept in the same repository and every projects codes in an other.\n# Every virtual environment will be in ... mkdir ~/.ve # Every projects will be in ... mkdir ~/Documents/Python/workspace  We have to configure the shell to initialise pyenv at the opening of the terminal. Thus, we have to add the following lines into the file ~/.bashrc.\nexport WORKON_HOME=~/.ve export PROJECT_HOME=~/Documents/Python/workspace eval \u0026quot;$(pyenv init -)\u0026quot;  Reload the terminal to take the changes into account.\nNext step is to install CPython 3.7.1 and CPython 2.7.15.\npyenv install 3.7.1 pyenv install 2.7.15  Configure the global Python installation It is nice to use Python written programs without using a virtual environment. Moreover, it is easier if we only have one Jupyter Notebook/Lab, one iPython console for Python 2, one iPython console for Python 3 and other tools.\nSo, we use pyenv-virtualenv to do that:\npyenv virtualenv 3.7.1 jupyter3 pyenv virtualenv 3.7.1 tools3 pyenv virtualenv 2.7.15 ipython2 pyenv virtualenv 2.7.15 tools2  Jupyter can handle with many kernels like Python 2, Python 3, R, bash, and some other. It allows only one Jupyter installation.\n Here, we just want to use Python2 and Python3.\n Let\u0026rsquo;s start with Python3:\npyenv activate jupyter3 pip install jupyter pip install jupyterlab python -m ipykernel install --user pyenv deactivate  Let\u0026rsquo;s continue with Python2:\npyenv activate ipython2 pip install ipykernel python -m ipykernel install --user pyenv deactivate   Note that when we install Jupyter for Python3, we install by default iPython and the kernel. For Python2, we only need to install iPython and the kernel.\n Now, let\u0026rsquo;s install tools using Python3:\npyenv activate tools3 pip install youtube-dl rows pyenv deactivate  Let\u0026rsquo;s install tools which do not work with Python3 but only with Python2:\npyenv activate tools2 pip install rename pyenv deactivate  Finally, it is time to let all the Python versions and the special virtual environments working together.\npyenv global 3.7.1 2.7.15 jupyter3 ipython2 tools3 tools2   Note that this command put priority in the $PATH environment variable. Thus, it is possible to reach the scripts without activating virtual environments.\n Using virtual environment We use pyenv-virtualenvwrapper to create the virtual environment for each project. Now, he have to add the line pyenv virtualenvwrapper_lazy in the file ~/.bashrc and then reload the terminal. When we start a new session, pyenv-virtualenvwrapper will install the necessary dependencies of virtualenvwrapper if they are not here. It is possible to use commands from virtualenvwrapper and every virtual environment will be created by using Python implementations installed from pyenv.\nSome examples:\n  Let\u0026rsquo;s say I want a new project proj3 using Python3. The command mkproject proj3 will create a new virtual environment using Python3 (by default) in the repository ~/.ve/proj3 and a project repository ~/Documents/Python/workspace/proj3.\n  Let\u0026rsquo;s imagine I want to work on my project proj3. Run the command workon proj3 will activate the virtual environment ~/.ve/proj3 and change the working directory to ~/Documents/Python/workspace/proj3.\n  Let\u0026rsquo;s clone a project names proj2 in the directory ~/Documents/Python/workspace/proj2. So, I need a virtual environment for this project. Run the command mkvirtualenv -a ~/Documents/Python/workspace/proj2 -p python2 proj2 will create a virtual environment using Python2 in the directory ~/.ve/proj2 linked to the project. Then, run workon proj2 will activate the virtual environment and change the working directory.\n  Using Jupyter and iPython with the projects  At the beginning, Jupyter and Console were parts of iPython project which was only about Python. But the evolution of the Notebook allows to use more languages than just Python. So, the developers decide to split the project: Jupyter and iPython. Now, Notebook is part of Jupyter and Console is part of iPython and the Python kernel used by Jupyter to launch Python code.\n So, Jupyter do not detect the active virtual environment: it is the iPython instance that Jupyter initialize. The problem is that the iPython virtual environment launches itself only in interactive shell mode and not in kernel mode. Otherwise, the code detection works correctly only if the Python version of the active virtual environment and the Python version launches by iPython are the same.\nThe solution is to customize the process of the iPython start-up. To do that, we need a iPython profile et launch a script :\nipython profile create curl -L http://hbn.link/hb-ipython-startup-script \u0026gt; ~/.ipython/profile_default/startup/00-venv-sitepackages.py  So, no matter the mode in which iPython is launched, the site-packages of the virtual environment will be available in the PYTHONPATH.\nBack to proj3, after run workon proj3, it is possible to execute iPython to be in the interactive mode, or jupyter-notebook to use the notebook.\nUpdating the package with pip In order to update the different Python packages, run the following command:\npip list --outdated | cut -d \u0026quot; \u0026quot; -f 1 | xargs -n1 pip install -U  ","date":1544054400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544054400,"objectID":"852a01db0db4befab9e08f7dfa059d7d","permalink":"/post/set-up-python/","publishdate":"2018-12-06T00:00:00Z","relpermalink":"/post/set-up-python/","section":"post","summary":"Python is a programming language. It is used by a lot of people doing data science.","tags":["Python","Set up"],"title":"Set up Python","type":"post"},{"authors":null,"categories":["R"],"content":"Installation  R is very easy to install on MacOS.\nThe first thing to do is to add R to the available formulae in Homebrew. And then, install R.\nbrew tap homebrew/science brew install r  It could be necessary to install XQuartz to use R (but it is also possible that it is installed by default)\nbrew cask install xquartz  A nice GUI to use with R is Rstudio.\nbrew cask install rstudio  Jupyter kernel The installation of the R kernel for Jupyter is straightforward following this link .\nOn MacOS, from a Terminal, run R to launch a R session. Then, run the following commands:\ninstall.packages('IRKernel') # Install the package IRKernel::installspec() # Make Jupyter to see the R kernel  Configuration of the proxies How to install packages if you have to deal with proxies? First, you should know the repository where R is installed.\nR.home()  And then, you have to add to the file ${R_HOME}/etc/Renviron the lines:\nhttp_proxy=http://\u0026lt;user_name\u0026gt;:\u0026lt;password\u0026gt;@\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/ https_proxy=https://\u0026lt;user_name\u0026gt;:\u0026lt;password\u0026gt;@\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/ ftp_proxy=ftp://\u0026lt;user_name\u0026gt;:\u0026lt;password\u0026gt;@\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/  Some modification to functions Summary functions  Dataframe summary  ```{r} summary_df if(any(sapply(df, class) == \u0026lsquo;factor\u0026rsquo;)){ result$Factor \u0026lt;- df %\u0026gt;% select_if(is.factor) %\u0026gt;% imap(summary_column) } if(any(sapply(df, class) == \u0026lsquo;numeric\u0026rsquo;)){ result$Numeric \u0026lt;- df %\u0026gt;% select_if(is.numeric) %\u0026gt;% imap_dfr(summary_column) }\nreturn(result) }\n\u0026lt;/p\u0026gt; \u0026lt;/details\u0026gt;\t\u0026lt;details\u0026gt; \u0026lt;summary\u0026gt;Column dataframe summary\u0026lt;/summary\u0026gt; \u0026lt;p\u0026gt; ```{r} summary_column \u0026lt;- function(df.column, name.column){ # Function that get a column from a dataframe and return statistics on it. # Depending on the column class, the results will not be the same. if(class(df.column) == 'factor'){ colName \u0026lt;- name.column df.column %\u0026gt;% fct_count() %\u0026gt;% rename(!!colName := f, Count = n) } else if(class(df.column) == 'numeric'){ tibble( Name = name.column, NA_num = sum(is.na(df.column)), Unique = length(unique(df.column)), Range = max(df.column, na.rm = TRUE) - min(df.column, na.rm = TRUE), Mean = round(mean(df.column, na.rm = TRUE), digits = 2), Variance = round(var(df.column, na.rm = TRUE), digits = 2), Minimum = min(df.column, na.rm = TRUE), Q05 = quantile(df.column, probs = .05, na.rm = TRUE), Q10 = quantile(df.column, probs = .10, na.rm = TRUE), Q25 = quantile(df.column, probs = .25, na.rm = TRUE), Q50 = quantile(df.column, probs = .50, na.rm = TRUE), Q75 = quantile(df.column, probs = .75, na.rm = TRUE), Q90 = quantile(df.column, probs = .90, na.rm = TRUE), Q95 = quantile(df.column, probs = .95, na.rm = TRUE), Maximum = max(df.column, na.rm = TRUE) ) } }   Print functions  Print summary dataframe  ```{r} print_summary_df \")) cat(glue::glue(\" **{names(l)[i]} variables** \\n\\n\")) if(class(l[[i]]) == 'list'){ for(j in seq_along(l[[i]])){ cat(glue::glue(\"\")) cat(glue::glue(\" {names(l[[i]][j])} \\n\\n\")) cat('\\n') l[[i]][j] %% kable(format = 'html') %% kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), position = \"center\") %% print() cat('\\n\\n') } } else{ cat('\\n') l[[i]] %% kable(format = 'html') %% kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), position = \"center\") %% print() cat('\\n') } cat(glue::glue(\"\\n\")) } } \u0026lt;/p\u0026gt; \u0026lt;/details\u0026gt; \u0026lt;details\u0026gt; \u0026lt;summary\u0026gt;Print dataframe\u0026lt;/summary\u0026gt; \u0026lt;p\u0026gt; ```{r} print_df \u0026lt;- function(l){ # Print function for dataframe to be renderer in html. cat('\u0026lt;div style=\u0026quot;overflow-x:auto;\u0026quot;\u0026gt;\\n') l %\u0026gt;% kable(format = 'html') %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;), position = \u0026quot;center\u0026quot;) %\u0026gt;% print() cat('\u0026lt;/div\u0026gt;\u0026lt;/ul\u0026gt;\\n\\n') }    Print summary lm  ```{r} print_summary_lm cat(glue::glue(\u0026ldquo;\u0026rdquo;))\nPrint the formula cat(glue::glue(\u0026ldquo; Formula: \u0026ldquo;, \u0026ldquo;{deparse(lm_summary$call$formula)} \u0026rdquo;))\nTreat the residuals cat(glue::glue(\u0026ldquo; Residuals \\n\u0026rdquo;)) cat(\u0026lsquo;\\n\u0026rsquo;) lm_summary$residuals %\u0026gt;% summary_column(name.column = \u0026lsquo;Residuals\u0026rsquo;) %\u0026gt;% kable(format = \u0026lsquo;html\u0026rsquo;, digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026ldquo;striped\u0026rdquo;, \u0026ldquo;hover\u0026rdquo;, \u0026ldquo;condensed\u0026rdquo;, \u0026ldquo;responsive\u0026rdquo;), position = \u0026ldquo;center\u0026rdquo;) %\u0026gt;% print() cat(\u0026lsquo;\\n\\n\u0026rsquo;)\nTreat the regression coefficient cat(glue::glue(\u0026ldquo; Coefficients \\n\u0026rdquo;)) cat(\u0026lsquo;\\n\u0026rsquo;) coef \u0026lt;- lm_summary$coefficients coef[, \u0026lsquo;Pr(\u0026gt;|t|)'] \u0026lt;- format.pval(coef[, \u0026lsquo;Pr(\u0026gt;|t|)']) coef \u0026lt;- coef %\u0026gt;% as.data.frame(stringsAsFactors = FALSE) %\u0026gt;% rownames_to_column(\u0026lsquo;Variable\u0026rsquo;) %\u0026gt;% as.tibble() %\u0026gt;% map_at(c(\u0026ldquo;Estimate\u0026rdquo;, \u0026ldquo;Std. Error\u0026rdquo;, \u0026ldquo;t value\u0026rdquo;), as.numeric) coef %\u0026gt;% as.tibble() %\u0026gt;% kable(format = \u0026lsquo;html\u0026rsquo;, digits = 5) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026ldquo;striped\u0026rdquo;, \u0026ldquo;hover\u0026rdquo;, \u0026ldquo;condensed\u0026rdquo;, \u0026ldquo;responsive\u0026rdquo;), position = \u0026ldquo;center\u0026rdquo;) %\u0026gt;% print() cat(\u0026lsquo;\\n\\n\u0026rsquo;)\nTreat other stats pval \u0026lt;- format.pval(pf(lm_summary$fstatistic[1L], lm_summary$fstatistic[2L], lm_summary$fstatistic[3L], lower.tail = FALSE), digits = 3) cat(glue::glue(\u0026ldquo; Residual standard error: \u0026ldquo;, \u0026ldquo;{round(lm_summary$sigma, 3)} on {lm_summary$df[2]} degrees of freedom. \u0026rdquo;)) cat(glue::glue(\u0026ldquo; Multiple $R^2$: {round(lm_summary$r.squared, 3)}.\u0026rdquo;)) cat(glue::glue(\u0026ldquo; Adjusted $R^2$: {round(lm_summary$adj.r.squared, 3)}.\u0026rdquo;)) cat(glue::glue(\u0026ldquo; F-statistic: \u0026ldquo;, \u0026ldquo;{round(lm_summary$fstatistic[1L], 3)} on {lm_summary$fstatistic[2L]} and {lm_summary$fstatistic[3L]}, \u0026ldquo;, \u0026ldquo;p-value: {pval}. \u0026rdquo;))\ncat(glue::glue(\u0026ldquo;\u0026rdquo;)) }\n\u0026lt;/p\u0026gt; \u0026lt;/details\u0026gt; \u0026lt;details\u0026gt; \u0026lt;summary\u0026gt;Print summary glm\u0026lt;/summary\u0026gt; \u0026lt;p\u0026gt; ```{r} print_summary_glm \u0026lt;- function(glm_summary){ cat(glue::glue(\u0026quot;Results of the model on the **{glm_summary$call$data}** dataset.\\n\u0026quot;)) cat(glue::glue(\u0026quot;\u0026lt;ul\u0026gt;\u0026quot;)) # Print the formula cat(glue::glue(\u0026quot;\u0026lt;li\u0026gt; *Formula*: \u0026quot;, \u0026quot;{deparse(glm_summary$call$formula)} \u0026lt;/li\u0026gt;\u0026quot;)) # Treat the residuals cat(glue::glue(\u0026quot;\u0026lt;li\u0026gt; *Residuals* \u0026lt;/li\u0026gt;\\n\u0026quot;)) cat('\u0026lt;div style=\u0026quot;overflow-x:auto;\u0026quot;\u0026gt;\\n') glm_summary$deviance.resid %\u0026gt;% summary_column(name.column = 'Residuals') %\u0026gt;% kable(format = 'html', digits = 2) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;), position = \u0026quot;center\u0026quot;) %\u0026gt;% print() cat('\u0026lt;/div\u0026gt;\\n\\n') # Treat the regression coefficient cat(glue::glue(\u0026quot;\u0026lt;li\u0026gt; *Coefficients* \u0026lt;/li\u0026gt;\\n\u0026quot;)) cat('\u0026lt;div style=\u0026quot;overflow-x:auto;\u0026quot;\u0026gt;\\n') coef \u0026lt;- glm_summary$coefficients coef[, 'Pr(\u0026gt;|z|)'] \u0026lt;- format.pval(coef[, 'Pr(\u0026gt;|z|)']) coef \u0026lt;- coef %\u0026gt;% as.data.frame(stringsAsFactors = FALSE) %\u0026gt;% rownames_to_column('Variable') %\u0026gt;% as.tibble() %\u0026gt;% map_at(c(\u0026quot;Estimate\u0026quot;, \u0026quot;Std. Error\u0026quot;, \u0026quot;z value\u0026quot;), as.numeric) coef %\u0026gt;% as.tibble() %\u0026gt;% kable(format = 'html', digits = 5) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;, \u0026quot;responsive\u0026quot;), position = \u0026quot;center\u0026quot;) %\u0026gt;% print() cat('\u0026lt;/div\u0026gt;\\n\\n') # Treat other stats cat(glue::glue(\u0026quot;\u0026lt;li\u0026gt; *Null deviance*: \u0026quot;, \u0026quot;{round(glm_summary$null.deviance, 3)} on {glm_summary$df.null} degrees of freedom. \u0026lt;/li\u0026gt;\u0026quot;)) cat(glue::glue(\u0026quot;\u0026lt;li\u0026gt; *Residual deviance*: \u0026quot;, \u0026quot;{round(glm_summary$deviance, 3)} on {glm_summary$df.residual} degrees of freedom. \u0026lt;/li\u0026gt;\u0026quot;)) cat(glue::glue(\u0026quot;\u0026lt;li\u0026gt; *AIC*: \u0026quot;, \u0026quot;{round(glm_summary$aic, 3)}\u0026lt;/li\u0026gt;\u0026quot;)) cat(glue::glue(\u0026quot;\u0026lt;/ul\u0026gt;\\n\u0026quot;)) }   Plot functions  Plot confusion matrix  ```{r} plot_confusion_matrix % as.data.frame(optional = TRUE) %% rownames_to_column() %% rename('Var1' = '.') %% ggplot() + geom_text(aes(x = Var1, y = Var2, label = Freq), size = 4) + xlab('Prediction') + ylab('True') + geom_hline(aes(yintercept = 1.5), size = 0.2) + geom_vline(aes(xintercept = 1.5), size = 0.2) + theme_bw() + scale_x_discrete(position = \"top\") + theme(panel.grid = element_blank(), axis.ticks = element_blank()) } ```   Plot regsubset summary  ```{r} ggregsubsets % mutate(rsq = 100*rsq) if(\"adjr2\" %in% criterion) df % mutate(adjr2 = 100*adjr2) df \u0026lt;- df %\u0026gt;% gather(variable, is_in, -criterion, -nvars) %\u0026gt;% gather(measure, value, -nvars, -variable, -is_in)\nif(\u0026ldquo;rsq\u0026rdquo; %in% criterion) df[df[\u0026lsquo;measure\u0026rsquo;] == \u0026lsquo;rsq\u0026rsquo;, \u0026lsquo;measure\u0026rsquo;] \u0026lt;- \u0026lsquo;$R^2$\u0026rsquo; if(\u0026ldquo;rss\u0026rdquo; %in% criterion) df[df[\u0026lsquo;measure\u0026rsquo;] == \u0026lsquo;rss\u0026rsquo;, \u0026lsquo;measure\u0026rsquo;] \u0026lt;- \u0026lsquo;$RSS$\u0026rsquo; if(\u0026ldquo;adjr2\u0026rdquo; %in% criterion) df[df[\u0026lsquo;measure\u0026rsquo;] == \u0026lsquo;adjr2\u0026rsquo;, \u0026lsquo;measure\u0026rsquo;] \u0026lt;- \u0026lsquo;Adjusted $R^2$\u0026rsquo; if(\u0026ldquo;cp\u0026rdquo; %in% criterion) df[df[\u0026lsquo;measure\u0026rsquo;] == \u0026lsquo;cp\u0026rsquo;, \u0026lsquo;measure\u0026rsquo;] \u0026lt;- \u0026lsquo;$C_p$\u0026rsquo; if(\u0026ldquo;bic\u0026rdquo; %in% criterion) df[df[\u0026lsquo;measure\u0026rsquo;] == \u0026lsquo;bic\u0026rsquo;, \u0026lsquo;measure\u0026rsquo;] \u0026lt;- \u0026lsquo;$BIC$\u0026rsquo;\np \u0026lt;- ggplot(df, aes(variable, factor(round(value)))) + geom_tile(aes(fill = is_in)) + facet_wrap(~ measure, scales = \u0026ldquo;free\u0026rdquo;) + scale_fill_manual(\u0026quot;\u0026quot;, values = c(\u0026ldquo;TRUE\u0026rdquo; = \u0026ldquo;black\u0026rdquo;, \u0026ldquo;FALSE\u0026rdquo; = \u0026ldquo;white\u0026rdquo;), guide = FALSE) + labs(x = \u0026ldquo;\u0026quot;, y = \u0026ldquo;\u0026quot;) return(p) }\n\u0026lt;/p\u0026gt; \u0026lt;/details\u0026gt; \u0026lt;details\u0026gt; \u0026lt;summary\u0026gt;Plot criteria for model selection\u0026lt;/summary\u0026gt; \u0026lt;p\u0026gt; ```{r} ggcriteria \u0026lt;- function(x, criterion = \u0026quot;bic\u0026quot;){ require(dplyr); require(ggplot2); require(tidyr) if(inherits(x, \u0026quot;regsubsets\u0026quot;)) x \u0026lt;- summary(x) if(!inherits(x, \u0026quot;summary.regsubsets\u0026quot;)) stop(\u0026quot;The input to ggregsubsets() should be the result of regsubsets().\u0026quot;) if(\u0026quot;rsq\u0026quot; == criterion) crit \u0026lt;- '$R^2$' if(\u0026quot;rss\u0026quot; == criterion) crit \u0026lt;- '$RSS$' if(\u0026quot;adjr2\u0026quot; == criterion) crit \u0026lt;- 'Adjusted $R^2$' if(\u0026quot;cp\u0026quot; == criterion) crit \u0026lt;- '$C_p$' if(\u0026quot;bic\u0026quot; == criterion) crit \u0026lt;- '$BIC$' if((criterion == \u0026quot;adjr2\u0026quot;) | (criterion == \u0026quot;rsq\u0026quot;)) m \u0026lt;- which.max(x[[criterion]]) else m \u0026lt;- which.min(x[[criterion]]) p \u0026lt;- ggplot() + geom_line(aes(x = seq(1, length(x[[criterion]])), y = x[[criterion]])) + geom_point(aes(x = m, y = x[[criterion]][m]), col = 'red', size = 3) + xlab('Number of variables') + scale_x_continuous(breaks = seq(1, length(x[[criterion]])), minor_breaks = NULL) + ylab(crit) return(p) }    Plot cross-validation error from `cv.glmnet` function  ```{r} ggcv.glmnet df \u0026lt;- tibble(lambda = log(x$lambda), cvm = x$cvm, cvsd = x$cvsd) p \u0026lt;- ggplot(df, aes(lambda, cvm, ymin = cvm - cvsd, ymax = cvm + cvsd)) + geom_point(col = \u0026lsquo;red\u0026rsquo;) + geom_errorbar() + geom_vline(aes(xintercept = df$lambda[which.min(df$cvm)]), col = \u0026lsquo;blue\u0026rsquo;, linetype = 2) + xlab(\u0026lsquo;$\\log(\\lambda)$\u0026rsquo;) + ylab(\u0026lsquo;Mean-Squared Error\u0026rsquo;) return(p) }\n\u0026lt;/p\u0026gt; \u0026lt;/details\u0026gt;  ","date":1543968000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543968000,"objectID":"fa62c0b0f47df556f86e097570617713","permalink":"/post/set-up-r/","publishdate":"2018-12-05T00:00:00Z","relpermalink":"/post/set-up-r/","section":"post","summary":"R is a software for statistical computing and graphics. It is very easy to install on macOS.","tags":["R","Set up"],"title":"Set up R","type":"post"},{"authors":null,"categories":["R","Machine Learning"],"content":"Context The sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n Variables description    Variable Name Description     PassengerId Passenger’s Id   Survived Survived (1) or died (0)   Pclass Passenger’s class   Name Passenger’s name   Sex Passenger’s sex   Age Passenger’s age   SibSp Number of siblings/spouses aboard   Parch Number of parents/children aboard   Ticket Ticket number   Fare Passenger Fare   Cabin Cabin   Embarked Port of embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)    SPECIAL NOTES:\nPclass is a proxy for socio-economic status (SES): 1st ~ Upper; 2nd ~ Middle; 3rd ~ Lower.\nAge is in Years; fractional if Age is less than One (1). If the Age is estimated, it is in the form xx.5.\nWith respect to the family relation variables (i.e. sibsp and parch) some relations were ignored. The following are the definitions used for sibsp and parch:\n Sibling: Brother, Sister, Stepbrother, or Stepsister of Passenger Aboard Titanic; Spouse: Husband or Wife of Passenger Aboard Titanic (Mistresses and Fiances Ignored); Parent: Mother or Father of Passenger Aboard Titanic; Child: Son, Daughter, Stepson, or Stepdaughter of Passenger Aboard Titanic.  Other family relatives excluded from this study include cousins, nephews/nieces, aunts/uncles, and in-laws. Some children travelled only with a nanny, therefore parch = 0 for them. As well, some travelled with very close friends or neighbors in a village, however, the definitions do not support such relations.\nLoad the data train \u0026lt;- read_csv('train.csv') test \u0026lt;- read_csv('test.csv') titanic \u0026lt;- train %\u0026gt;% bind_rows(test) %\u0026gt;% select(-PassengerId) %\u0026gt;% mutate_at(vars(Pclass, Sex, Embarked), funs(factor(.)))  The train dataset has 891 observations and 12 variables. The test dataset has 418 observations and 11 variables. We want to use the train dataset to learn if a passenger survived given the different variables, and then predict the fate of the passenger into the test dataset.\nExploratory Data Analysis Passenger’s class There is no missing values into the PClass variable. Half of the passenger are in the third class.\nPassenger’s sex There is almost twice men than women.\nPassenger’s name This variable, obviously, confirm the high number of men compare to the number of women. But it carry another piece of information: more than the half of the women on the Titanic are not married (the Miss factor). It is probably due to the children.\n# Extract the title from the Passenger's name. Title \u0026lt;- \u0026quot;^.*, (.*?)\\\\..*$\u0026quot; %\u0026gt;% gsub(\u0026quot;\\\\1\u0026quot;, titanic$Name) # Create another factors for low represented title. title_high \u0026lt;- c('Mr', 'Miss', 'Mrs', 'Master') Title \u0026lt;- Title %in% title_high %\u0026gt;% if_else(Title, 'Other') # Add titlecolumn to the dataframe titanic \u0026lt;- titanic %\u0026gt;% add_column(Title) %\u0026gt;% mutate_at(vars(Title), funs(factor(.)))  Passenger’s port of embarkation 1 % of the passengers embarked in Southampton. We do not known the port of embarkation for only 2 persons. So, we will try to infer these missing values.\nFirst, let’s take a look at the 2 passengers with missing port of embarkation.\n  Survived  Pclass  Name  Sex  Age  SibSp  Parch  Ticket  Fare  Cabin  Embarked  Title      1  1  Icard, Miss. Amelie  female  38  0  0  113572  80  B28  NA  Miss    1  1  Stone, Mrs. George Nelson (Martha Evelyn)  female  62  0  0  113572  80  B28  NA  Mrs     Miss. Icard and Mrs. Stone paid 80$ and was in first class. Let’s plot a boxplot to determine the median fare depending on the port of embarkation for the first class.\nThere are only 3 passengers that embarked in Queenstown in first class. There fare was 90$. Moreover, they were part of the same family. So, considering the boxplot, we might think that the port of embarkation of Miss. Icard and Mrs. Stone were Cherbourg.\ntitanic[62, \u0026quot;Embarked\u0026quot;] \u0026lt;- \u0026quot;C\u0026quot; titanic[830, \u0026quot;Embarked\u0026quot;] \u0026lt;- \u0026quot;C\u0026quot;  Passenger’s fare There is only 1 person with a missing in the all dataset. The mean fare is 33$ and the median fare 14$ for a ticket on the Titanic. Let’s look at the person with a missing fare.\n  Survived  Pclass  Name  Sex  Age  SibSp  Parch  Ticket  Fare  Cabin  Embarked  Title      NA  3  Storey, Mr. Thomas  male  60.5  0  0  3701  NA  NA  S  Mr     Let’s plot a kernel density estimator of the fare for the person with the same characteristics than Mr. Storey (embarked in Southampton in third class).\nThe median for the third class and the embarkment in Southampton is 8$. So, we might think that Mr. Storey has paid the median fare of the people from the third class who embarked in Southampton.\ntitanic[1044, \u0026quot;Fare\u0026quot;] \u0026lt;- titanic %\u0026gt;% filter(Embarked == 'S', Pclass == 3) %\u0026gt;% pull(Fare) %\u0026gt;% median(na.rm = TRUE)  Passenger’s age There are 263 persons without Age in the dataset. The mean age is 29.9 years old.\nSince there are a lot of missing values, we are going to input these ones using a ridge regression ( glmnet_ package ).\n# Split the dataset into the ones with Age and the ones without Age. titanic.with.age \u0026lt;- titanic %\u0026gt;% filter(!is.na(Age)) %\u0026gt;% select(-c(Survived, Name, Ticket, Cabin)) titanic.without.age \u0026lt;- titanic %\u0026gt;% filter(is.na(Age)) %\u0026gt;% select(-c(Survived, Name, Ticket, Cabin)) %\u0026gt;% mutate(Age = 0) # Build a model matrix of the data titanic.lm \u0026lt;- lm(Age ~ ., data = titanic.with.age) titanic.with.age.model.matrix \u0026lt;- model.matrix(titanic.lm, data = titanic.with.age)[,-1] # Perform the Ridge Regression (alpha = 0) titanic.age.model \u0026lt;- glmnet(titanic.with.age.model.matrix, titanic.with.age$Age, alpha = 0) # Prediction of the Age titanic.without.age$Age \u0026lt;- predict(titanic.age.model, newx = model.matrix(titanic.lm, data = titanic.without.age)[, -1], s = cv.glmnet(titanic.with.age.model.matrix, titanic.with.age$Age, alpha = 0)$lambda.min, type = 'link') # Replace the missing Age into the all dataset titanic[is.na(titanic$Age), \u0026quot;Age\u0026quot;] \u0026lt;- titanic.without.age$Age  Let’s check the new density estimator for the Age to ensure that things still look good. (Careful, one person with a predicted negative age!)\nNumber of siblings/spouses aboard There is no missing value for the variable SipSp in the dataset. A majority if the passengers does not have siblings or spouses aboard.\nNumber of parents/children aboard There is no missing value for the variable Parch in the dataset. A majority if the passengers does not have parents or children aboard.\nPassenger’s cabin There are 1014 missing values for the Cabin variable. So, 77% of the observations are missing. We decided to delete this features from the dataset.\ntitanic \u0026lt;- titanic %\u0026gt;% select(-Cabin)  Passenger’s ticket There are 0 missing values for the Ticket variable. But, there are 929 different values. Thus, we also delete this feature from the dataset because almost every passenger has a different Ticket.\ntitanic \u0026lt;- titanic %\u0026gt;% select(-Ticket)  Prediction of the survivors train \u0026lt;- titanic %\u0026gt;% select(-Name) %\u0026gt;% filter(!is.na(Survived)) test \u0026lt;- titanic %\u0026gt;% select(-Name) %\u0026gt;% filter(is.na(Survived)) # Split the train set into two dataset (for validation) set.seed(42) sample \u0026lt;- sample(c(TRUE, FALSE), nrow(train), replace = TRUE, prob = c(2/3, 1/3)) train.val \u0026lt;- train[sample, ] test.val \u0026lt;- train[!sample, ] # Perform Ridge regression train.lm \u0026lt;- lm(Survived ~ ., data = train.val) X \u0026lt;- model.matrix(train.lm, data = train.val)[ , -1] Y \u0026lt;- train.val$Survived train.ridge.model \u0026lt;- glmnet(X, Y, alpha = 0, family = 'binomial') # Prediction on the test.val set test.val.predict \u0026lt;- predict(train.ridge.model, s = cv.glmnet(X, Y, alpha = 0)$lambda.min, newx = model.matrix(train.lm, data = test.val)[ , -1], type = 'class')  On the validation set, there are 0.14% of missclassified passengers.\n# Prediction of the test set test$Survived \u0026lt;- 0 test.predict \u0026lt;- predict(train.ridge.model, s = cv.glmnet(X, Y, alpha = 0)$lambda.min, newx = model.matrix(train.lm, data = test)[ , -1], type = 'class') # Construt the dataframe result \u0026lt;- data.frame(PassengerID = row.names(test.predict), Survived = test.predict[ , 1]) # Export as CSV write.csv(result, 'results.csv', row.names = FALSE)  ","date":1542844800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542844800,"objectID":"e3a3e1c6f8e63d4194ca77d41680cb9b","permalink":"/project/titanic/","publishdate":"2018-11-22T00:00:00Z","relpermalink":"/project/titanic/","section":"project","summary":"The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.","tags":["R","Machine Learning","Logistic Regression"],"title":"Titanic: Machine Learning from Disaster","type":"project"}]