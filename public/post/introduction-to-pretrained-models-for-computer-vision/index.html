<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.7.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Steven Golovkine">

  
  
  
    
  
  <meta name="description" content="In particular, we are going to use the ResNet50, MobileNet and InceptionResNetV2 models for Image Classification and Mask-RCNN models for Instance Detection and Segmentation.">

  
  <link rel="alternate" hreflang="en-us" href="/post/introduction-to-pretrained-models-for-computer-vision/">

  


  
  
  
  <meta name="theme-color" content="#795548">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Cutive+Mono%7CLora:400,700%7CRoboto:400,700&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu1d2d3b6ebbfe3e5882ce5725380dc208_3894_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu1d2d3b6ebbfe3e5882ce5725380dc208_3894_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="/post/introduction-to-pretrained-models-for-computer-vision/">

  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@StevenGolovkine">
  <meta property="twitter:creator" content="@StevenGolovkine">
  
  <meta property="og:site_name" content="TwentyFourSecs">
  <meta property="og:url" content="/post/introduction-to-pretrained-models-for-computer-vision/">
  <meta property="og:title" content="Introduction to Pretrained Models for Computer Vision | TwentyFourSecs">
  <meta property="og:description" content="In particular, we are going to use the ResNet50, MobileNet and InceptionResNetV2 models for Image Classification and Mask-RCNN models for Instance Detection and Segmentation."><meta property="og:image" content="/post/introduction-to-pretrained-models-for-computer-vision/featured.png">
  <meta property="twitter:image" content="/post/introduction-to-pretrained-models-for-computer-vision/featured.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2019-08-14T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2019-08-14T00:00:00&#43;00:00">
  

  


    






  






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/post/introduction-to-pretrained-models-for-computer-vision/"
  },
  "headline": "Introduction to Pretrained Models for Computer Vision",
  
  "image": [
    "/post/introduction-to-pretrained-models-for-computer-vision/featured.png"
  ],
  
  "datePublished": "2019-08-14T00:00:00Z",
  "dateModified": "2019-08-14T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Steven Golovkine"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "TwentyFourSecs",
    "logo": {
      "@type": "ImageObject",
      "url": "img//"
    }
  },
  "description": "In particular, we are going to use the ResNet50, MobileNet and InceptionResNetV2 models for Image Classification and Mask-RCNN models for Instance Detection and Segmentation."
}
</script>

  

  


  


  





  <title>Introduction to Pretrained Models for Computer Vision | TwentyFourSecs</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">TwentyFourSecs</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">TwentyFourSecs</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#featured"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item">
        <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
      </li>
      

      

    </ul>

  </div>
</nav>


  <article class="article">

  




















  
  


<div class="article-container pt-3">
  <h1>Introduction to Pretrained Models for Computer Vision</h1>

  

  


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Aug 14, 2019
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    15 min read
  </span>
  

  
  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/categories/deep-learning/">Deep Learning</a>, <a href="/categories/python/">Python</a></span>
  

</div>

  














</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 640px; max-height: 360px;">
  <div style="position: relative">
    <img src="/post/introduction-to-pretrained-models-for-computer-vision/featured.png" alt="" class="featured-image">
    <span class="article-header-caption">Photo by Jon Flobrant on Unsplash</span>
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      <h1 id="introduction-to-pretrained-models-for-computer-vision">Introduction to Pretrained Models for Computer Vision</h1>
<p>This notebook is based on the Deep Learning course from the Master Datascience Paris Saclay. Materials of the course can be found 
<a href="https://github.com/m2dsupsdlclass/lectures-labs" target="_blank" rel="noopener">here</a>
.</p>
<p>It aims to get some hands-on experience with pre-trained Keras models are reasonably close to the state-of-the-art of some computer vision tasks. The models are pre-trained on large publicly available labeled images datasets such as 
<a href="http://www.image-net.org/" target="_blank" rel="noopener">ImageNet</a>
 and 
<a href="http://cocodataset.org/#home" target="_blank" rel="noopener">COCO</a>
.</p>
<p>This notebook will highlights two specific tasks (or at least try):</p>
<ul>
<li><strong>Image Classification</strong>: Predict only one class label per-image (assuming a single centered object or image class).</li>
<li><strong>Object detection and instance segmentation</strong>: Detect and localise all occurrences of objects of a predefined list of classes of interest in a given image.</li>
</ul>
<pre><code class="language-python"># Display figure in the notebook
%matplotlib inline
</code></pre>
<pre><code class="language-python"># Load packages
import cv2
import matplotlib.pyplot as plt
import numpy as np
import os
import zipfile

from keras import backend
from keras.applications import inception_resnet_v2, mobilenet, resnet50

from skimage.io import imread
from skimage.transform import resize

from time import time
from urllib.request import urlretrieve
</code></pre>
<h2 id="working-with-images-data">Working with images data</h2>
<p>For the beginning, we will see how to work with images data, how they are represented in memory, how to load it, how to modify it, <em>ect.</em></p>
<p>Let&rsquo;s use the library 
<a href="https://scikit-image.org/" target="_blank" rel="noopener"><code>scikit-image</code></a>
 to load the content of a JPEG file into a numpy array.</p>
<pre><code class="language-python">pic = imread('laptop.jpeg')
</code></pre>
<pre><code>The type of the variable is: &lt;class 'imageio.core.util.Array'&gt; (which is a particular class of np.ndarray).
</code></pre>
<p>An image is described by three parameters: its height; its weight; its color channels (RGB). Each of them corresponds to a dimension of the array.</p>
<pre><code>The shape of the picture is (450, 800, 3).
</code></pre>
<p>For efficiency reasons, the pixel intensities of each channel are stored as <strong>8-bit unsigned integer</strong> taking values in the <strong>$[0, 255]$ range</strong>.</p>
<pre><code>The type of the elements in the array is uint8, the minimum value of the pixels is 0 and the maximum value is 255.
</code></pre>
<pre><code class="language-python"># Look at the picture
fig = plt.imshow(pic)
fig.axes.get_xaxis().set_visible(False)
fig.axes.get_yaxis().set_visible(False)
plt.show()
</code></pre>
<p><img src="output_10_0.png" alt="png"></p>
<p>The size in bytes of a Numpy array can be computed by multiplying the number of elements by the size in byte of each element in the array. The size of one element depend of the data type.</p>
<pre><code>Using the shape, the size of the image is 1.08MB, the computation is 450 (height) * 800 (weight) * 3 (channel) * 8 (# bits to represents one element) / 8 (# bits in one byte).
We can check this results using the function `nbytes`: 1.08MB.
</code></pre>
<p>Indexing on the last dimension makes it possible to extract the 2D content of a specific color channel. Consider the following example for the red channel.</p>
<pre><code class="language-python">pic_red = pic[:, :, 0]
</code></pre>
<pre><code class="language-python">fig = plt.imshow(pic_red, cmap=plt.cm.Reds_r)
fig.axes.get_xaxis().set_visible(False)
fig.axes.get_yaxis().set_visible(False)
plt.show()
</code></pre>
<p><img src="output_15_0.png" alt="png"></p>
<p>Now, we consider the grey-level version of the image with shape <code>(height, weight)</code>. To compute this version, we compute the mean of each pixel values across the channels.</p>
<pre><code class="language-python">pic_grey = pic.mean(axis=2)
</code></pre>
<pre><code class="language-python">fig = plt.imshow(pic_grey, cmap=plt.cm.Greys_r)
fig.axes.get_xaxis().set_visible(False)
fig.axes.get_yaxis().set_visible(False)
plt.show()
</code></pre>
<p><img src="output_18_0.png" alt="png"></p>
<p>The <code>uint8</code> integer data type can not represent the grey pixels because they have floating points.  Anyway, Numpy represents it as <code>float64</code> data type.</p>
<pre><code>The size of the grey picture is 2.88MB (which is higher than the color picture, it is due to the `float` data type.
</code></pre>
<p>The expected of range values for the new pixels is the same as before, <strong>$[0, 255]$</strong> (0 for white pixels and 255 for black ones).</p>
<h3 id="resizing-images-handling-data-types-and-dynamic-ranges">Resizing images, handling data types and dynamic ranges</h3>
<p>When dealing with an heterogeneous collection of image of various sizes, it is often necessary to resize the image to the same size. More specifically:</p>
<ul>
<li>for <strong>image classification</strong>, most networks expect a specific <strong>fixed input size</strong>;</li>
<li>for <strong>object detection</strong> and instance segmentation, networks have more flexibility but the images should have <strong>approximately the same size as the training set images</strong>.</li>
</ul>
<p>Furthermore, <strong>large images can be much slower to process</strong> than smaller images. This is due to the fact that the number of pixels varies quadratically with the height and width.</p>
<pre><code class="language-python"># Resize the picture to have a 50*50 picture.
pic = imread('laptop.jpeg')
pic_lowres = resize(pic, output_shape=(50, 50), mode='reflect', anti_aliasing=True)
</code></pre>
<pre><code class="language-python">fig = plt.imshow(pic_lowres, interpolation='nearest')
fig.axes.get_xaxis().set_visible(False)
fig.axes.get_yaxis().set_visible(False)
plt.show()
</code></pre>
<p><img src="output_24_0.png" alt="png"></p>
<p>The values of the pixels of the low resolution image are computed by combining the values of the pixels in the high resolution image. The result is therefore represented as floating points.</p>
<pre><code>The type of the elements of the array is float64, and the size of the image is 0.06MB.
</code></pre>
<p><strong>Careful</strong>, by conventions, both <code>skimage.transform.imresize</code> and <code>plt.imshow</code> assume that floating point values range from $0.0$ to $1.0$ when using floating points as opposed to $0$ to $255$ when using 8-bit integers.</p>
<pre><code>So, the range of pixel values of the low resolution image is [0.0, 0.996].
</code></pre>
<p>Note that Keras on the other hand might expect images encoded with values in the $[0.0, 255.0]$ range irrespectively of the data type of the array. To avoid the implicit conversion to the $[0.0, 1.0]$ range, we can use the <code>preserve_range=True</code> option in the <code>resize</code> function. But the <em>dtype</em> will change to <em>float64</em>.</p>
<pre><code class="language-python">pic_lowres_larran = resize(pic, output_shape=(50, 50), mode='reflect', anti_aliasing=True, preserve_range=True)
</code></pre>
<pre><code>And, the range of pixel values of the low resolution image with `preserve_range=True` is [0.0, 254.0].
</code></pre>
<p><strong>Warning</strong>: The behavior of <code>plt.imshow</code> depends on both the <em>dtype</em> and the dynamic range when displaying RGB images. In particular, it does not work on RGB images with <em>float64</em> values in the $[0.0, 255.0]$ range.</p>
<pre><code class="language-python">fig = plt.imshow(pic_lowres_larran, interpolation='nearest')
fig.axes.get_xaxis().set_visible(False)
fig.axes.get_yaxis().set_visible(False)
plt.show()
</code></pre>
<p><img src="output_33_0.png" alt="png"></p>
<p>For correctly displaying an RGB array with floating point values in the $[0.0, 255.0]$ range, we can divide all the values by $255$ or change the <em>dtype</em> to integers.</p>
<pre><code class="language-python">fig = plt.imshow(pic_lowres_larran / 255.0, interpolation='nearest')
fig.axes.get_xaxis().set_visible(False)
fig.axes.get_yaxis().set_visible(False)
plt.show()
</code></pre>
<p><img src="output_35_0.png" alt="png"></p>
<pre><code class="language-python">fig = plt.imshow(pic_lowres_larran.astype(np.uint8), interpolation='nearest')
fig.axes.get_xaxis().set_visible(False)
fig.axes.get_yaxis().set_visible(False)
plt.show()
</code></pre>
<p><img src="output_36_0.png" alt="png"></p>
<h3 id="optional-taking-snapshots-from-the-webcam">(Optional) Taking snapshots from the webcam</h3>
<p>We are going to use the 
<a href="https://github.com/skvark/opencv-python" target="_blank" rel="noopener">Python API of OpenCV</a>
 in order to take pictures.</p>
<pre><code class="language-python"># Define a function to take a snapshot 
def take_snapshot(camera_id=0, fallback_filename=None):
    camera = cv2.VideoCapture(camera_id)
    try:
        # Take 10 consecutive snapshots to let the camera automatically
        # tune itself and hope that the contrast and lightning of the
        # last snapshot is good enough.
        for i in range(10):
            snapshot_ok, image = camera.read()
        if snapshot_ok:
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        else:
            print('WARNING: Could not access the camera!')
            if fallback_filename:
                image = imread(fallback_filename)
    finally:
        camera.release()
    return image
</code></pre>
<pre><code class="language-python"># Test if the function take_snapshot is working.
pic = take_snapshot(camera_id=0, fallback_filename='laptop.jpeg')
</code></pre>
<h2 id="image-classification">Image Classification</h2>
<p>The Keras library includes several neural network models pretrained on the ImageNet classification dataset. A popular model that show a good tradeoff between computation speed, model size and accuracy is called <em>ResNet-50</em> (
<a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">here</a>
 for the article on Deep Residual Networks).</p>
<pre><code class="language-python"># Get the ResNet-50 model
model = resnet50.ResNet50(weights='imagenet')
</code></pre>
<p>Let&rsquo;s check that Tensorflow backend used by Keras as the default backend expect the color channel on the last axis. If it had not been the case, it would have been possible to change the order of the axes with <code>pic = pic.transpose(2, 0, 1)</code>.</p>
<pre><code>The color channel is on the channels_last.

The network has been trained on (224, 224) RGB images.
</code></pre>
<p><code>None</code> is used by Keras to mark dimensions with a dynamic number of elements. Here, <code>None</code> is the <em>batch size</em>, that is the number of images that can be processed at one. In the following, we will process only image at a time.</p>
<pre><code class="language-python"># Load and resize the picture
pic = imread('laptop.jpeg')
pic_224 = resize(pic, (224, 224), preserve_range=True, mode='reflect')
</code></pre>
<pre><code>The shape of the picture is (224, 224, 3), and the dtype of its elements is float64.
</code></pre>
<p>However, the model use <em>float32</em> dtype. So, we have to convert the picture into <em>float32</em>.</p>
<pre><code class="language-python">pic_224 = pic_224.astype(np.float32)
</code></pre>
<pre><code class="language-python">fig = plt.imshow(pic_224 / 255, interpolation='nearest')
fig.axes.get_xaxis().set_visible(False)
fig.axes.get_yaxis().set_visible(False)
plt.show()
</code></pre>
<p><img src="output_50_0.png" alt="png"></p>
<p>Note that the image has been deformed by the resizing. In practice, this should not degrade the performance of the network too much. There are two alternatives solutions to that problems:</p>
<ul>
<li>
<p>resizing the image so that the smallest side is set to 224;</p>
</li>
<li>
<p>extracting a square centered crop of size $(224, 224)$ from the resulting image.</p>
<p>The shape of the picture is (224, 224, 3), whereas the input shape of the model should be (None, 224, 224, 3). So we have to expand the dimension of the picture.</p>
</li>
</ul>
<pre><code class="language-python">pic_224_batch = pic_224[None, ...] # or np.expand_dims(pic_224, axis=0)
</code></pre>
<p><code>pic_224_batch</code> is now compatible with the input shape of the neural network, so let&rsquo;s make a prediction.</p>
<pre><code class="language-python">%%time
X = resnet50.preprocess_input(pic_224_batch.copy())
pred = model.predict(X)
</code></pre>
<pre><code>CPU times: user 1.79 s, sys: 52 ms, total: 1.84 s
Wall time: 1.49 s
</code></pre>
<p>Note that we make a copy each time as the function <code>preprocess_input</code> can modify the image inplace to reuse memory when preprocessing large datasets.</p>
<p>The output predictions are a 2D array with:</p>
<ul>
<li>One row per image in the batch;</li>
<li>One column per target class in the ImageNet LSVRC dataset ($1000$ possible classes) with the probabilities that a given image belongs to a particular class. Obviously, the sum of the columns for each row is equal to $1$.</li>
</ul>
<h3 id="decoding-the-predicition-probabilities">Decoding the predicition probabilities</h3>
<p>Reading the raw probabilities for the $1000$ possible ImageNet classes is tedious. Fortunately, Keras comes with an helper function to extract the highest rated classes according to the model and display both class names and the wordnet synset identifiers.</p>
<pre><code class="language-python">print('Predicted image labels:')
for class_id, class_name, confidence in resnet50.decode_predictions(pred, top=5)[0]:
    print(f&quot;\t* {class_name} (synset: {class_id}): {confidence}&quot;)
</code></pre>
<pre><code>Predicted image labels:
    * notebook (synset: n03832673): 0.3599511384963989
    * laptop (synset: n03642806): 0.25687211751937866
    * desk (synset: n03179701): 0.15139059722423553
    * mouse (synset: n03793489): 0.11147501319646835
    * desktop_computer (synset: n03180011): 0.051331911236047745
</code></pre>
<p>Check on the ImageNet 
<a href="https://www.image-net.org/" target="_blank" rel="noopener">website</a>
 to better understand the use of the terms <em>notebook</em> in the training set. Note that the network is not too confident about the class of the main object in that image. If we were to merge the <em>notebook</em> and the <em>laptop</em> classes, the prediction will be good.</p>
<p>Furthermore, the network also considers secondary objects (desk, mouse, &hellip;) but the model as been trained as an image (multiclass) classification model with a single expected class per image rather than a multi-label classification model such as an object detection model with several positive labels per image.</p>
<p>We have to keep that in mind when trying to make use of the predictions of such a model for a practical application. This is a fundamental limitation of the label structure of the training set.</p>
<h3 id="a-note-on-preprocessing">A note on preprocessing</h3>
<p>All Keras pretrained vision models expect images with <code>float32</code> dtype and values in the $[0, 255]$ range. When training neural network, it often works better to have values closer to zero.</p>
<ul>
<li>A typical preprocessing is to center each of the channel and normalize its variance.</li>
<li>Another one is to measure the <code>min</code> and the <code>max</code> values and to shift and rescale to the $(-1, 1)$ range.</li>
</ul>
<p>The exact kind of preprocessing is not very important, but it&rsquo;s very important to <strong>always reuse the preprocessing function that was used when training the model</strong>.</p>
<p>Now, we are going to use different model, <code>ResNet50</code>, <code>MobileNet</code> and <code>InceptionResNetV2</code>, to classify the images from Wikipedia. We can then compare the models in both time and prediction accuracy.</p>
<p>We start be defining some function to predict the object in the images.</p>
<pre><code class="language-python">def classify_resnet50(model, fallback_filename=None):
    &quot;&quot;&quot;
    Function that takes a snapshot of the webcam and display it
    along with the decoded prediction of the model and their
    confidence level.
    &quot;&quot;&quot;
    # Take a snapshot
    pic = take_snapshot(camera_id=0, fallback_filename=fallback_filename)
    
    # Preprocess the picture
    pic_224 = resize(pic, (224, 224), preserve_range=True, mode='reflect')
    pic_224_batch = pic_224[None, ...]
    
    # Do predictions
    pred = model.predict(resnet50.preprocess_input(pic_224_batch.copy()))
    
    # Show the pic
    fig = plt.imshow(pic / 255, interpolation='nearest')
    fig.axes.get_xaxis().set_visible(False)
    fig.axes.get_yaxis().set_visible(False)
    plt.show()
    
    # Print the decoded predictions
    print('Predicted image labels:')
    for class_id, class_name, confidence in resnet50.decode_predictions(pred, top=5)[0]:
        print(f&quot;\t* {class_name} (synset: {class_id}): {confidence}&quot;)
        
def classify_mobilenet(model, fallback_filename=None):
    &quot;&quot;&quot;
    Function that takes a snapshot of the webcam and display it
    along with the decoded prediction of the model and their
    confidence level.
    &quot;&quot;&quot;
    # Take a snapshot
    pic = take_snapshot(camera_id=0, fallback_filename=fallback_filename)
    
    # Preprocess the picture
    pic_224 = resize(pic, (224, 224), preserve_range=True, mode='reflect')
    pic_224_batch = pic_224[None, ...]
    
    # Do predictions
    pred = model.predict(mobilenet.preprocess_input(pic_224_batch.copy()))
    
    # Show the pic
    fig = plt.imshow(pic / 255, interpolation='nearest')
    fig.axes.get_xaxis().set_visible(False)
    fig.axes.get_yaxis().set_visible(False)
    plt.show()
    
    # Print the decoded predictions
    print('Predicted image labels:')
    for class_id, class_name, confidence in mobilenet.decode_predictions(pred, top=5)[0]:
        print(f&quot;\t* {class_name} (synset: {class_id}): {confidence}&quot;)
        
def classify_inception_resnet_v2(model, fallback_filename=None):
    &quot;&quot;&quot;
    Function that takes a snapshot of the webcam and display it
    along with the decoded prediction of the model and their
    confidence level.
    &quot;&quot;&quot;
    # Take a snapshot
    pic = take_snapshot(camera_id=0, fallback_filename=fallback_filename)
    
    # Preprocess the picture
    pic_299 = resize(pic, (299, 299), preserve_range=True, mode='reflect')
    pic_299_batch = pic_299[None, ...]
    
    # Do predictions
    pred = model.predict(inception_resnet_v2.preprocess_input(pic_299_batch.copy()))
    
    # Show the pic
    fig = plt.imshow(pic / 255, interpolation='nearest')
    fig.axes.get_xaxis().set_visible(False)
    fig.axes.get_yaxis().set_visible(False)
    plt.show()
    
    # Print the decoded predictions
    print('Predicted image labels:')
    for class_id, class_name, confidence in inception_resnet_v2.decode_predictions(pred, top=5)[0]:
        print(f&quot;\t* {class_name} (synset: {class_id}): {confidence}&quot;)
</code></pre>
<pre><code class="language-bash">%%bash
wget https://upload.wikimedia.org/wikipedia/commons/3/3f/JPEG_example_flower.jpg
wget https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Alcatel_one_touch_easy.jpg/450px-Alcatel_one_touch_easy.jpg
wget https://upload.wikimedia.org/wikipedia/commons/thumb/8/8d/President_Barack_Obama.jpg/480px-President_Barack_Obama.jpg
</code></pre>
<p>We will begin the models comparison using the model <code>ResNet50</code>.</p>
<pre><code class="language-python"># Get the ResNet50 model
model_resnet50 = resnet50.ResNet50(weights='imagenet')
</code></pre>
<pre><code class="language-python">%%time
classify_resnet50(model, 'JPEG_example_flower.jpg')
</code></pre>
<p><img src="output_67_1.png" alt="png"></p>
<pre><code>Predicted image labels:
    * pot (synset: n03991062): 0.37311553955078125
    * hair_slide (synset: n03476684): 0.08061367273330688
    * strawberry (synset: n07745940): 0.06821887940168381
    * vase (synset: n04522168): 0.05385832488536835
    * ant (synset: n02219486): 0.044536933302879333
CPU times: user 740 ms, sys: 112 ms, total: 852 ms
Wall time: 326 ms
</code></pre>
<pre><code class="language-python">%%time
classify_resnet50(model, '450px-Alcatel_one_touch_easy.jpg')
</code></pre>
<p><img src="output_68_1.png" alt="png"></p>
<pre><code>Predicted image labels:
    * cellular_telephone (synset: n02992529): 0.9453017711639404
    * radio (synset: n04041544): 0.022191418334841728
    * hand-held_computer (synset: n03485407): 0.02068745717406273
    * combination_lock (synset: n03075370): 0.0010540278162807226
    * pay-phone (synset: n03902125): 0.001006232458166778
CPU times: user 716 ms, sys: 48 ms, total: 764 ms
Wall time: 307 ms
</code></pre>
<pre><code class="language-python">%%time
classify_resnet50(model, '480px-President_Barack_Obama.jpg')
</code></pre>
<p><img src="output_69_1.png" alt="png"></p>
<pre><code>Predicted image labels:
    * groom (synset: n10148035): 0.7057098746299744
    * suit (synset: n04350905): 0.2643233835697174
    * gown (synset: n03450230): 0.009134724736213684
    * Windsor_tie (synset: n04591157): 0.008021678775548935
    * Loafer (synset: n03680355): 0.007970282807946205
CPU times: user 736 ms, sys: 40 ms, total: 776 ms
Wall time: 310 ms
</code></pre>
<pre><code class="language-python"># Get the MobileNet model
model_mobilenet = mobilenet.MobileNet(weights='imagenet')
</code></pre>
<pre><code class="language-python">%%time
classify_mobilenet(model_mobilenet, 'JPEG_example_flower.jpg')
</code></pre>
<p><img src="output_71_1.png" alt="png"></p>
<pre><code>Predicted image labels:
    * lemon (synset: n07749582): 0.3409530222415924
    * orange (synset: n07747607): 0.2852575182914734
    * sulphur_butterfly (synset: n02281406): 0.1666731983423233
    * strawberry (synset: n07745940): 0.09244516491889954
    * whistle (synset: n04579432): 0.01351318508386612
CPU times: user 1.72 s, sys: 52 ms, total: 1.77 s
Wall time: 1.56 s
</code></pre>
<pre><code class="language-python">%%time
classify_mobilenet(model_mobilenet, '450px-Alcatel_one_touch_easy.jpg')
</code></pre>
<p><img src="output_72_1.png" alt="png"></p>
<pre><code>Predicted image labels:
    * cellular_telephone (synset: n02992529): 0.5218481421470642
    * hand-held_computer (synset: n03485407): 0.4698248505592346
    * radio (synset: n04041544): 0.004865806549787521
    * dial_telephone (synset: n03187595): 0.0011262071784585714
    * remote_control (synset: n04074963): 0.0005011822795495391
CPU times: user 420 ms, sys: 12 ms, total: 432 ms
Wall time: 204 ms
</code></pre>
<pre><code class="language-python">%%time
classify_mobilenet(model_mobilenet, '480px-President_Barack_Obama.jpg')
</code></pre>
<p><img src="output_73_1.png" alt="png"></p>
<pre><code>Predicted image labels:
    * groom (synset: n10148035): 0.6955855488777161
    * Windsor_tie (synset: n04591157): 0.17594707012176514
    * suit (synset: n04350905): 0.09634945541620255
    * organ (synset: n03854065): 0.006460004951804876
    * theater_curtain (synset: n04418357): 0.006234230939298868
CPU times: user 412 ms, sys: 40 ms, total: 452 ms
Wall time: 225 ms
</code></pre>
<p>Now, let&rsquo;s fit the <code>InceptionResNetV2</code> model.</p>
<pre><code class="language-python"># Get the InceptionResNetV2 model
model_inception_resnet_v2 = inception_resnet_v2.InceptionResNetV2(weights='imagenet')
</code></pre>
<pre><code class="language-python">%%time
classify_inception_resnet_v2(model_inception_resnet_v2, 'JPEG_example_flower.jpg')
</code></pre>
<p><img src="output_76_1.png" alt="png"></p>
<pre><code>Predicted image labels:
    * lemon (synset: n07749582): 0.659601628780365
    * orange (synset: n07747607): 0.10027674585580826
    * pot (synset: n03991062): 0.09215866029262543
    * strawberry (synset: n07745940): 0.012752575799822807
    * sulphur_butterfly (synset: n02281406): 0.010476206429302692
CPU times: user 7.52 s, sys: 120 ms, total: 7.64 s
Wall time: 6.52 s
</code></pre>
<pre><code class="language-python">%%time
classify_inception_resnet_v2(model_inception_resnet_v2, '450px-Alcatel_one_touch_easy.jpg')
</code></pre>
<p><img src="output_77_1.png" alt="png"></p>
<pre><code>Predicted image labels:
    * cellular_telephone (synset: n02992529): 0.8715960383415222
    * hand-held_computer (synset: n03485407): 0.013546744361519814
    * radio (synset: n04041544): 0.004702822770923376
    * modem (synset: n03777754): 0.004240750335156918
    * combination_lock (synset: n03075370): 0.0029042132664471865
CPU times: user 1.59 s, sys: 92 ms, total: 1.68 s
Wall time: 574 ms
</code></pre>
<pre><code class="language-python">%%time
classify_inception_resnet_v2(model_inception_resnet_v2, '480px-President_Barack_Obama.jpg')
</code></pre>
<p><img src="output_78_1.png" alt="png"></p>
<pre><code>Predicted image labels:
    * suit (synset: n04350905): 0.39843690395355225
    * groom (synset: n10148035): 0.3309420049190521
    * Windsor_tie (synset: n04591157): 0.06947924941778183
    * theater_curtain (synset: n04418357): 0.04553558677434921
    * bow_tie (synset: n02883205): 0.004989056382328272
CPU times: user 1.58 s, sys: 68 ms, total: 1.64 s
Wall time: 574 ms
</code></pre>
<h2 id="instance-detection-and-segmentation-with-mask-rcnn">Instance Detection and Segmentation with Mask-RCNN</h2>
<p>
<a href="https://arxiv.org/pdf/1703.06870.pdf" target="_blank" rel="noopener">Mask-RCNN</a>
 is a refinement of the 
<a href="https://arxiv.org/pdf/1506.01497.pdf" target="_blank" rel="noopener">Faster-RCNN</a>
 <strong>object detection</strong> model to also add support for <strong>instance segmentation</strong>. The following shows how to use a 
<a href="https://github.com/matterport/Mask_RCNN" target="_blank" rel="noopener">Keras based implementation</a>
 provided by 
<a href="https://matterport.com/" target="_blank" rel="noopener">Matterport</a>
 along with model parameters pretrained on the  
<a href="http://cocodataset.org/#home" target="_blank" rel="noopener">COCO Object Detection dataset</a>
.</p>
<pre><code class="language-python"># Download the model and the COCO trained weights
URL = &quot;https://github.com/ogrisel/Mask_RCNN/archive/master.zip&quot;
FOLDER = 'maskrcnn'
FILENAME = 'Mask_RCNN-master.zip'
if not os.path.exists(FOLDER):
    if not os.path.exists(FILENAME):
        tic = time()
        print(f'Downloading {URL} to {FILENAME} (can take a couple of minutes)...')
        urlretrieve(URL, FILENAME)
        print(f'Done in {time() - tic}')
    print(f'Extracting archive to {FOLDER}...')
    zipfile.ZipFile(FILENAME).extractall('.')
    os.rename('Mask_RCNN-master', FOLDER)

COCO_MODEL_FILE = 'mask_rcnn_coco.h5'
if not os.path.exists(COCO_MODEL_FILE):
    from maskrcnn import utils
    print('Pretrained model can take several minutes to download.')
    utils.download_trained_weights(COCO_MODEL_FILE)
</code></pre>
<h3 id="create-model-and-load-training-weights">Create Model and Load Training Weights</h3>
<pre><code class="language-python">from maskrcnn import config
from maskrcnn import model as modellib

class InferenceCocoConfig(config.Config):
    # Give the configuration a recognizable name
    NAME = 'inference_coco'
    
    # Number of classes (including background)
    NUM_CLASSES = 1 + 80 # COCO has 80 classes.
    
    # Set batch size to 1 since we'll be running inference on
    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU
    GPU_COUNT = 1
    IMAGES_PER_GPU = 1
    
    
config = InferenceCocoConfig()
model = modellib.MaskRCNN(mode='inference', model_dir='mask_rcnn/logs', config=config)

# Load weights trained on MS-COCO
COCO_MODEL_FILE = 'mask_rcnn_coco.h5'
model.load_weights(COCO_MODEL_FILE, by_name=True)
</code></pre>
<h3 id="class-names">Class Names</h3>
<p>Index of the class in the list is its ID. For example, to get ID of the teddy bear class, use: <code>class_names.index('teddy bear')</code>. <code>BG</code> stands for background.</p>
<pre><code class="language-python"># COCO class names
class_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',
               'bus', 'train', 'truck', 'boat', 'traffic light',
               'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',
               'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',
               'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',
               'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',
               'kite', 'baseball bat', 'baseball glove', 'skateboard',
               'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',
               'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',
               'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',
               'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',
               'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',
               'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',
               'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',
               'teddy bear', 'hair drier', 'toothbrush']
</code></pre>
<h3 id="run-object-detection">Run Object Detection</h3>
<p>Let&rsquo;s perform object segmentation on an image taken on the web.</p>
<pre><code class="language-bash">%%bash
wget https://storage.needpix.com/rsynced_images/street-scene-2301158_1280.jpg
</code></pre>
<pre><code class="language-python"># Read the image
pic = imread('street-scene-2301158_1280.jpg')
</code></pre>
<pre><code class="language-python">fig = plt.imshow(pic, interpolation='nearest')
fig.axes.get_xaxis().set_visible(False)
fig.axes.get_yaxis().set_visible(False)
plt.show()
</code></pre>
<p><img src="output_88_0.png" alt="png"></p>
<pre><code class="language-python">from maskrcnn import visualize

# Run detection
tic = time()
results = model.detect([pic], verbose=1)
toc = time()
print(f'Image analyzed in {np.around(toc - tic, 2)} secondes.')
</code></pre>
<pre><code>Processing 1 images
image                    shape: (960, 1280, 3)        min:    0.00000  max:  255.00000
molded_images            shape: (1, 1024, 1024, 3)    min: -123.70000  max:  150.11562
image_metas              shape: (1, 89)               min:    0.00000  max: 1280.00000
Image analyzed in 13.57 secondes.
</code></pre>
<pre><code class="language-python"># Visualize the results
r = results[0] # Take the results for the first image.
for class_id, score in zip(r['class_ids'], r['scores']):
    print(f'{class_names[class_id]}:\t{score}')
</code></pre>
<pre><code>car:	0.9879944324493408
car:	0.9879814982414246
car:	0.9828053116798401
car:	0.9798809289932251
car:	0.9787180423736572
car:	0.9613178968429565
car:	0.9486448168754578
car:	0.9470494389533997
motorcycle:	0.9209350943565369
motorcycle:	0.9195521473884583
car:	0.9188504815101624
car:	0.9100474119186401
car:	0.9096055626869202
car:	0.8942683339118958
car:	0.8911943435668945
person:	0.8910031914710999
person:	0.8883078098297119
bus:	0.8868105411529541
car:	0.8834719061851501
person:	0.8810924291610718
car:	0.8774976134300232
car:	0.870510995388031
bus:	0.8356407284736633
motorcycle:	0.8196616172790527
bus:	0.8100437521934509
person:	0.8088014721870422
person:	0.8027144074440002
person:	0.8002358675003052
car:	0.7951725721359253
car:	0.7892614006996155
person:	0.787919282913208
motorcycle:	0.7864757776260376
car:	0.7821618318557739
person:	0.771487832069397
person:	0.7644047141075134
bus:	0.7624491453170776
person:	0.7461680769920349
car:	0.7306007742881775
car:	0.7271723747253418
car:	0.7174354195594788
car:	0.7103606462478638
</code></pre>
<pre><code class="language-python"># Visualization on the picture
visualize.display_instances(pic, r['rois'], r['masks'], 
                            r['class_ids'],
                            class_names, r['scores'])
</code></pre>
<p><img src="output_91_0.png" alt="png"></p>

    </div>

    



<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/deep-learning/">Deep Learning</a>
  
  <a class="badge badge-light" href="/tags/python/">Python</a>
  
  <a class="badge badge-light" href="/tags/keras/">Keras</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=/post/introduction-to-pretrained-models-for-computer-vision/&amp;text=Introduction%20to%20Pretrained%20Models%20for%20Computer%20Vision" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=/post/introduction-to-pretrained-models-for-computer-vision/&amp;t=Introduction%20to%20Pretrained%20Models%20for%20Computer%20Vision" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Introduction%20to%20Pretrained%20Models%20for%20Computer%20Vision&amp;body=/post/introduction-to-pretrained-models-for-computer-vision/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=/post/introduction-to-pretrained-models-for-computer-vision/&amp;title=Introduction%20to%20Pretrained%20Models%20for%20Computer%20Vision" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Introduction%20to%20Pretrained%20Models%20for%20Computer%20Vision%20/post/introduction-to-pretrained-models-for-computer-vision/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=/post/introduction-to-pretrained-models-for-computer-vision/&amp;title=Introduction%20to%20Pretrained%20Models%20for%20Computer%20Vision" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  






  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <img class="avatar mr-3 avatar-circle" src="/authors/admin/avatar_hucbc2930ba76141ffd5ffb026ad90ced3_85341_270x270_fill_q90_lanczos_center.jpg" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="/">Steven Golovkine</a></h5>
      <h6 class="card-subtitle">PhD student</h6>
      <p class="card-text">My research interests include functional data analysis, non-parametric statistics and machine learning.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:steven_golovkine@icloud.com" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/steven-golovkine/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/StevenGolovkine" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>









  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/post/introduction-to-deep-learning-with-keras/">Introduction to Deep Learning with Keras</a></li>
      
      <li><a href="/project/lower-back-pain/">Lower Back Pain</a></li>
      
      <li><a href="/post/jupyter-notebook/">Jupyter Notebook</a></li>
      
      <li><a href="/post/library-matplotlib/">Library Matplotlib</a></li>
      
      <li><a href="/post/set-up-python/">Set up Python</a></li>
      
    </ul>
  </div>
  



  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.0630fec5958cb075a5a38f042b3ddde6.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  
  <p class="powered-by">
    
      <a href="/privacy/">Privacy Policy</a>
    
    
       &middot; 
      <a href="/terms/">Terms</a>
    
  </p>
  

  <p class="powered-by">
    © 2020 Steven Golovkine. All Rights Reserved &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
